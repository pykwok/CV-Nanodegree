{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算机视觉纳米学位项目\n",
    "\n",
    "## 实战项目：图像标注\n",
    "\n",
    "---\n",
    "\n",
    "在这个notebook中，你要做的是训练你的CNN-RNN模型。\n",
    "\n",
    "我们欢迎并鼓励你在搜索好的模型时尝试多种不同的架构和超参数。\n",
    "\n",
    "这样的话，很有可能会使项目变得非常凌乱！所以，在提交项目之前，请确保清理以下内容：\n",
    "- 你在这个notebook上写的代码。该notebook应描述如何训练单个CNN-RNN架构，使其与你最终选择的超参数相对应。此外，你的notebook应便于审阅专家通过运行此notebook中的代码来复制结果。\n",
    "- **Step 2**中代码单元格的输出。这个输出显示的应该是从零开始训练模型时获得的输出。\n",
    "\n",
    "我们将会对这个notebook**进行评分**。\n",
    "\n",
    "你可以通过点击以下链接导航到该notebook：\n",
    "- [Step 1](#step1): 训练设置\n",
    "- [Step 2](#step2): 训练你的模型\n",
    "- [Step 3](#step3): (可选）验证你的模型\n",
    "\n",
    "<a id='step1'></a>\n",
    "## Step 1: 训练设置\n",
    "\n",
    "在该notebook的此步骤中，你需要通过定义超参数并设置训练过程中重要的其他选项来自定义对CNN-RNN模型的训练。在下面的**Step 2**中训练模型时，会使用到现在设置的值。\n",
    "\n",
    "请注意，你只可以修改以`TODO`语句开头的代码块。**对于所以不在`TODO`语句之前的代码块，不能做任何修改。**\n",
    "\n",
    "### 任务 #1\n",
    "\n",
    "首先，请设置以下变量：\n",
    "- `batch_size` - 每个训练批次的批次大小。它是指用于在每个训练步骤中修改模型权重的图像标注对的数量。\n",
    "- `vocab_threshold` - 单词阈值最小值。请注意，阈值越大，词汇量越小，而阈值越小，则表示将包括较少的词汇，词汇量则越大。\n",
    "- `vocab_from_file` - 一个布尔值，用于决定是否从文件加载词汇表。\n",
    "- `embed_size` - the dimensionality of the image and word embeddings.  图像和单词嵌入的维度。\n",
    "- `hidden_size` - RNN解码器隐藏状态下的特征数。\n",
    "- `num_epochs` - 训练模型的epoch数。我们建议你设置为`num_epochs=3`，但可以根据需要随意增加或减少此数字。 [这篇论文](https://arxiv.org/pdf/1502.03044.pdf) 在一个最先进的GPU上对一个标注生成模型训练了3天，但很快你就会发现，其实在几个小时内就可以得到合理的结果！（_但是，如果你想让你的模型与当前的研究一较高下，则需要更长时间的训练。_)\n",
    "- `save_every` - 确定保存模型权重的频率。我们建议你设置为`save_every=1`，便于在每个epoch后保存模型权重。这样，在第`i`个epoch之后，编码器和解码器权重将在`models/`文件夹中分别保存为`encoder-i.pkl`和`decoder-i.pkl`。\n",
    "- `print_every` - 确定在训练时将批次损失输出到Jupyter notebook的频率。请注意，训练时，你**将不会**看到损失函数的单调减少，这一点非常好并且完全可以预料到！我们建议你将其保持在默认值`100` ，从而避免让这个notebook运行变慢，但之后随时都可以进行更改。\n",
    "- `log_file` - 包含每个步骤中训练期间的损失与复杂度演变过程的的文本文件的名称。\n",
    "\n",
    "对于上述某些值，如果你不确定从哪里开始设置，可以仔细阅读 [这篇文章](https://arxiv.org/pdf/1502.03044.pdf) 与 [这篇文章](https://arxiv.org/pdf/1411.4555.pdf) ，获得有用的指导！为了避免在该notebook上花费太长时间，我们建议你查阅这些研究论文，从中可以获得有关哪些超参数可能最有效的初始猜测。然后，训练单个模型，然后继续下一个notebook（**3_Inference.ipynb**）。如果你对模型的效果不满意，可以返回此notebook调整超参数和/或**model.py**中的体系结构，并重新训练模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题1\n",
    "\n",
    "**问题:** 详细描述你的CNN-RNN架构。对于这种架构，任务1中变量的值，你是如何选择的？如果你查阅了某一篇详细说明关于成功实现图像标注生成模型的研究论文，请提供该参考论文。\n",
    "\n",
    "**答案:** \n",
    "- `vocab_threshold`是5和`embed_size`、`hidden_size`为512，这三个参数是引用了《Show and Tell: A Neural Image Caption Generator》论文。  \n",
    "- `vocab_from_file`选择了True，从文件中加载词汇表，减少加载时间。\n",
    "- `num_epochs`怕训练途中中断，选择了1。\n",
    "- `save_every`由于`num_epochs`是1，所以这个变量只能是1了。\n",
    "- `print_every`设置的是默认的100，可以比较好地观测loss的变化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （可选）任务 #2\n",
    "\n",
    "请注意，我们为你推荐了一个用于预处理训练图像的图像转换`transform_train`，但同时，也欢迎并鼓励你根据需要进行修改。修改此转换时，请牢记：\n",
    "- 数据集中的图像具有不同的高度和宽度\n",
    "- 如果使用预先训练的模型，则必须执行相应的相应归一化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题2\n",
    "\n",
    "**问题:** 你是如何在`transform_train`中选择转换方式的？如果你将转换保留为其提供的值，为什么你的任务它非常适合你的CNN架构？\n",
    "\n",
    "**答案:**   \n",
    "未修改提供的`transform_train`。因为上面的操作也是我自己会选择的操作。  \n",
    "由于我们的CNN提取器选择的是resnet，所以先把大小不一的图像resize为256×256的大小，再任意剪裁成224×224的大小。以0.5的概率随机水平翻转图片可使数据增广。再把图像转成tensor。最后用和ImageNet数据集的数据对图片normalize。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 任务 #3\n",
    "\n",
    "接下来，你需要定义一个包含模型的可学习参数的Python列表。 例如，如果你决定使解码器中的所有权重都是可训练的，但只想在编码器的嵌入层中训练权重，那么，就应该将`params`设置为："
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "params = list(decoder.parameters()) + list(encoder.embed.parameters()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题3\n",
    "\n",
    "**问题:** 你是如何选择该架构的可训练参数的？ 为什么你认为这是一个不错的选择？\n",
    "\n",
    "**答案:**  \n",
    "encoder的使用预先训练的ResNet-50架构（删除了最终的完全连接层）从一批预处理图像中提取特征。所以encoder只需要对我们新增的encoder.embed的参数进行训练。  \n",
    "decoder没有预训练参数，decoder的参数均需训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 任务 #4\n",
    "\n",
    "最后，选择一个 [优化程序](http://pytorch.org/docs/master/optim.html#torch.optim.Optimizer)。\n",
    "\n",
    "### 问题4\n",
    "\n",
    "**问题:** 你是如何选择用于训练模型的优化程序的？\n",
    "\n",
    "**答案:**  \n",
    "《Show and Tell: A Neural Image Caption Generator》论文选择了SGD，但是loss下降比较慢。   \n",
    "RMSprop在learning rate为0.01和0.001的loss下降也比较慢。  \n",
    "最后查阅《Show, Attend and Tell: Neural Image Caption Generation with Visual Attention》论文，建议在MSCOCO数据集上使用Adam。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import sys\n",
    "sys.path.append('/opt/cocoapi/PythonAPI')\n",
    "from pycocotools.coco import COCO\n",
    "from data_loader import get_loader\n",
    "from model import EncoderCNN, DecoderRNN\n",
    "import math\n",
    "\n",
    "\n",
    "## TODO #1: Select appropriate values for the Python variables below.\n",
    "batch_size = 64          # batch size\n",
    "vocab_threshold = 5        # minimum word count threshold\n",
    "vocab_from_file = True    # if True, load existing vocab file\n",
    "embed_size = 512           # dimensionality of image and word embeddings\n",
    "hidden_size = 512          # number of features in hidden state of the RNN decoder\n",
    "num_epochs = 1             # number of training epochs\n",
    "save_every = 1             # determines frequency of saving model weights\n",
    "print_every = 100          # determines window for printing average loss\n",
    "log_file = 'training_log.txt'       # name of file with saved training loss and perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary successfully loaded from vocab.pkl file!\n",
      "loading annotations into memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414113/414113 [01:38<00:00, 4223.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done (t=0.90s)\n",
      "creating index...\n",
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    }
   ],
   "source": [
    "# (Optional) TODO #2: Amend the image transform below.\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "# Build data loader.\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=vocab_from_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8855"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The size of the vocabulary.\n",
    "vocab_size = len(data_loader.dataset.vocab)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecoderRNN(\n",
       "  (embeddings): Embedding(8855, 512)\n",
       "  (lstm): LSTM(512, 512, batch_first=True, dropout=0.5)\n",
       "  (dropout): Dropout(p=0.5)\n",
       "  (fc): Linear(in_features=512, out_features=8855, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the encoder and decoder. \n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "# Move models to GPU if CUDA is available. \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function. \n",
    "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO #3: Specify the learnable parameters of the model.\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO #4: Define the optimizer.\n",
    "optimizer = torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the total number of training steps per epoch.\n",
    "total_step = math.ceil(len(data_loader.dataset.caption_lengths) / data_loader.batch_sampler.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer2 = torch.optim.RMSprop(params, lr=0.01, alpha=0.99, eps=1e-08)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2'></a>\n",
    "## Step 2: 训练你的模型\n",
    "\n",
    "在**Step 1**中执行代码单元格后，下面的训练过程应该就不会出现问题了。\n",
    "\n",
    "在这里，完全可以将代码单元格保留其原样，无需修改即可训练模型。但是，如果要修改用于训练下面模型的代码，则必须确保审阅专家能够很容易地看明白你的更改内容。换句话说，请务必提供适当的注释来描述代码的工作方式！\n",
    "\n",
    "你可能会发现，使用加载已保存的权重来恢复训练很有用。在这种情况下，请注意包含你要加载的编码器和解码器权重的文件的名称（`encoder_file`和`decoder_file`）。之后，你就可以使用下面的代码行加载权重："
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Load pre-trained weights before resuming training.\n",
    "encoder.load_state_dict(torch.load(os.path.join('./models', encoder_file)))\n",
    "decoder.load_state_dict(torch.load(os.path.join('./models', decoder_file)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在试验参数时，请务必记录大量笔记并记录你在各种训练中使用的设置。特别是，你不希望遇到这样的情况，即已经训练了几个小时的模型，但不记得使用的设置:)。\n",
    "\n",
    "### 关于调整超参数的说明\n",
    "\n",
    "为了弄清楚模型的运行情况，你可以尝试去了解训练过程中训练损失和复杂度是如何演变的。为了做好本项目，我们建议你根据这些信息修改超参数。\n",
    "\n",
    "但是，这样你还是无法知道模型是否过度拟合训练数据，但你要知道的是，过度拟合是训练图像标注模型时常会遇到的问题。\n",
    "\n",
    "对于这个项目，你不必担心过度拟合。**该项目对模型的性能没有严格的要求**，你只需要证明你的模型在生成基于测试数据的标注时学到了**_一些东西_**。现在，我们强烈建议你为我们建议的3个epoch训练你的模型，但不必担心性能；然后，立即转换到下一个notebook（**3_Inference.ipynb**），查看模型对测试数据的执行情况。如果你的模型需要更改，可以回到这个notebook，修改超参数（如有必要的话），并重新训练该模型。\n",
    "\n",
    "也就是说，如果你想在这个项目中有所超越，可以阅读 [本文](http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7505636)4.3.1节中最小化过度拟合的一些方法。在本notebook的下一个（可选）步骤中，我们提供了一些关于评估验证数据集性能的指导。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [100/6471], Loss: 4.2550, Perplexity: 70.4576\n",
      "Epoch [1/1], Step [200/6471], Loss: 3.9014, Perplexity: 49.47253\n",
      "Epoch [1/1], Step [300/6471], Loss: 3.4528, Perplexity: 31.5891\n",
      "Epoch [1/1], Step [400/6471], Loss: 3.2200, Perplexity: 25.0291\n",
      "Epoch [1/1], Step [500/6471], Loss: 3.1031, Perplexity: 22.26613\n",
      "Epoch [1/1], Step [600/6471], Loss: 2.9180, Perplexity: 18.5048\n",
      "Epoch [1/1], Step [700/6471], Loss: 2.9737, Perplexity: 19.5636\n",
      "Epoch [1/1], Step [800/6471], Loss: 2.8478, Perplexity: 17.2495\n",
      "Epoch [1/1], Step [900/6471], Loss: 3.3888, Perplexity: 29.6292\n",
      "Epoch [1/1], Step [1000/6471], Loss: 3.0405, Perplexity: 20.9147\n",
      "Epoch [1/1], Step [1100/6471], Loss: 2.9665, Perplexity: 19.4240\n",
      "Epoch [1/1], Step [1200/6471], Loss: 2.9501, Perplexity: 19.1073\n",
      "Epoch [1/1], Step [1300/6471], Loss: 2.8515, Perplexity: 17.3140\n",
      "Epoch [1/1], Step [1400/6471], Loss: 2.9473, Perplexity: 19.0553\n",
      "Epoch [1/1], Step [1500/6471], Loss: 2.7239, Perplexity: 15.23938\n",
      "Epoch [1/1], Step [1600/6471], Loss: 2.6791, Perplexity: 14.5726\n",
      "Epoch [1/1], Step [1700/6471], Loss: 2.9566, Perplexity: 19.2332\n",
      "Epoch [1/1], Step [1800/6471], Loss: 3.2498, Perplexity: 25.7851\n",
      "Epoch [1/1], Step [1900/6471], Loss: 2.7537, Perplexity: 15.7002\n",
      "Epoch [1/1], Step [2000/6471], Loss: 2.8012, Perplexity: 16.4651\n",
      "Epoch [1/1], Step [2100/6471], Loss: 2.6085, Perplexity: 13.5792\n",
      "Epoch [1/1], Step [2200/6471], Loss: 2.9662, Perplexity: 19.41866\n",
      "Epoch [1/1], Step [2300/6471], Loss: 2.8227, Perplexity: 16.8228\n",
      "Epoch [1/1], Step [2400/6471], Loss: 4.3751, Perplexity: 79.4479\n",
      "Epoch [1/1], Step [2500/6471], Loss: 2.5589, Perplexity: 12.9222\n",
      "Epoch [1/1], Step [2600/6471], Loss: 2.5066, Perplexity: 12.2627\n",
      "Epoch [1/1], Step [2700/6471], Loss: 3.1291, Perplexity: 22.8529\n",
      "Epoch [1/1], Step [2800/6471], Loss: 2.6388, Perplexity: 13.9957\n",
      "Epoch [1/1], Step [2900/6471], Loss: 2.6625, Perplexity: 14.3314\n",
      "Epoch [1/1], Step [3000/6471], Loss: 2.5659, Perplexity: 13.0128\n",
      "Epoch [1/1], Step [3100/6471], Loss: 2.6392, Perplexity: 14.0021\n",
      "Epoch [1/1], Step [3200/6471], Loss: 2.5026, Perplexity: 12.2142\n",
      "Epoch [1/1], Step [3300/6471], Loss: 2.5732, Perplexity: 13.1072\n",
      "Epoch [1/1], Step [3400/6471], Loss: 2.4276, Perplexity: 11.3321\n",
      "Epoch [1/1], Step [3500/6471], Loss: 2.5837, Perplexity: 13.2462\n",
      "Epoch [1/1], Step [3600/6471], Loss: 2.6198, Perplexity: 13.7335\n",
      "Epoch [1/1], Step [3700/6471], Loss: 2.5778, Perplexity: 13.1681\n",
      "Epoch [1/1], Step [3800/6471], Loss: 2.4408, Perplexity: 11.4826\n",
      "Epoch [1/1], Step [3900/6471], Loss: 2.7415, Perplexity: 15.5098\n",
      "Epoch [1/1], Step [4000/6471], Loss: 2.5171, Perplexity: 12.3930\n",
      "Epoch [1/1], Step [4100/6471], Loss: 2.1805, Perplexity: 8.85049\n",
      "Epoch [1/1], Step [4200/6471], Loss: 2.1905, Perplexity: 8.93968\n",
      "Epoch [1/1], Step [4300/6471], Loss: 2.5355, Perplexity: 12.6232\n",
      "Epoch [1/1], Step [4400/6471], Loss: 2.4863, Perplexity: 12.0169\n",
      "Epoch [1/1], Step [4500/6471], Loss: 3.0066, Perplexity: 20.2193\n",
      "Epoch [1/1], Step [4600/6471], Loss: 2.5029, Perplexity: 12.2180\n",
      "Epoch [1/1], Step [4700/6471], Loss: 2.4117, Perplexity: 11.15278\n",
      "Epoch [1/1], Step [4800/6471], Loss: 2.6267, Perplexity: 13.8276\n",
      "Epoch [1/1], Step [4900/6471], Loss: 2.4566, Perplexity: 11.6649\n",
      "Epoch [1/1], Step [5000/6471], Loss: 2.5740, Perplexity: 13.1182\n",
      "Epoch [1/1], Step [5100/6471], Loss: 2.3800, Perplexity: 10.8053\n",
      "Epoch [1/1], Step [5200/6471], Loss: 2.3796, Perplexity: 10.8008\n",
      "Epoch [1/1], Step [5300/6471], Loss: 2.3930, Perplexity: 10.9459\n",
      "Epoch [1/1], Step [5400/6471], Loss: 2.3588, Perplexity: 10.5778\n",
      "Epoch [1/1], Step [5500/6471], Loss: 2.4066, Perplexity: 11.0959\n",
      "Epoch [1/1], Step [5600/6471], Loss: 2.5530, Perplexity: 12.8459\n",
      "Epoch [1/1], Step [5700/6471], Loss: 2.3283, Perplexity: 10.2602\n",
      "Epoch [1/1], Step [5800/6471], Loss: 2.5281, Perplexity: 12.5301\n",
      "Epoch [1/1], Step [5900/6471], Loss: 2.4610, Perplexity: 11.7161\n",
      "Epoch [1/1], Step [6000/6471], Loss: 2.4664, Perplexity: 11.7795\n",
      "Epoch [1/1], Step [6100/6471], Loss: 2.2974, Perplexity: 9.94856\n",
      "Epoch [1/1], Step [6200/6471], Loss: 2.6086, Perplexity: 13.58024\n",
      "Epoch [1/1], Step [6300/6471], Loss: 2.4074, Perplexity: 11.1049\n",
      "Epoch [1/1], Step [6400/6471], Loss: 2.3418, Perplexity: 10.4001\n",
      "Epoch [1/1], Step [6471/6471], Loss: 2.4505, Perplexity: 11.5937"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Open the training log file.\n",
    "f = open(log_file, 'w')\n",
    "\n",
    "old_time = time.time()\n",
    "response = requests.request(\"GET\", \n",
    "                            \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/keep_alive_token\", \n",
    "                            headers={\"Metadata-Flavor\":\"Google\"})\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    \n",
    "    for i_step in range(1, total_step+1):\n",
    "        \n",
    "        if time.time() - old_time > 60:\n",
    "            old_time = time.time()\n",
    "            requests.request(\"POST\", \n",
    "                             \"https://nebula.udacity.com/api/v1/remote/keep-alive\", \n",
    "                             headers={'Authorization': \"STAR \" + response.text})\n",
    "        \n",
    "        # Randomly sample a caption length, and sample indices with that length.\n",
    "        indices = data_loader.dataset.get_train_indices()\n",
    "        # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "        new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        data_loader.batch_sampler.sampler = new_sampler\n",
    "        \n",
    "        # Obtain the batch.\n",
    "        images, captions = next(iter(data_loader))\n",
    "\n",
    "        # Move batch of images and captions to GPU if CUDA is available.\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        \n",
    "        # Zero the gradients.\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        \n",
    "        # Pass the inputs through the CNN-RNN model.\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions)\n",
    "        \n",
    "        # Calculate the batch loss.\n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "        \n",
    "        # Backward pass.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters in the optimizer.\n",
    "        optimizer.step()\n",
    "            \n",
    "        # Get training statistics.\n",
    "        stats = 'Epoch [%d/%d], Step [%d/%d], Loss: %.4f, Perplexity: %5.4f' % (epoch, num_epochs, i_step, total_step, loss.item(), np.exp(loss.item()))\n",
    "        \n",
    "        # Print training statistics (on same line).\n",
    "        print('\\r' + stats, end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        # Print training statistics to file.\n",
    "        f.write(stats + '\\n')\n",
    "        f.flush()\n",
    "        \n",
    "        # Print training statistics (on different line).\n",
    "        if i_step % print_every == 0:\n",
    "            print('\\r' + stats)\n",
    "            \n",
    "    # Save the weights.\n",
    "    if epoch % save_every == 0:\n",
    "        torch.save(decoder.state_dict(), os.path.join('./models', 'decoder-%d.pkl' % epoch))\n",
    "        torch.save(encoder.state_dict(), os.path.join('./models', 'encoder-%d.pkl' % epoch))\n",
    "\n",
    "# Close the training log file.\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3'></a>\n",
    "## Step 3: (可选）验证你的模型\n",
    "\n",
    "为了评估潜在的过度拟合，可以选择评估验证集的性能。如果你决定来做这个**可选**任务，则需要先完成下一个notebook中的所有步骤（**3_Inference.ipynb**）。作为该notebook的一部分，你需要编写并测试使用RNN解码器生成图像标注的代码（特别是`DecoderRNN`类中的`sample`方法）。在这里，该代码会是非常有用的。\n",
    "\n",
    "如果你决定验证模型，请不要在**data_loader.py**中编辑数据加载器。相反，你需要创建一个名为**data_loader_val.py**的新文件，其中包含用于获取验证数据的数据加载器的代码。你可以访问：\n",
    "-  路径为`'/opt/cocoapi/images/train2014/'`的验证图像文件\n",
    "-  路径为`'/opt/cocoapi/annotations/captions_val2014.json'`的文件中 ，用于验证图像标注的注释文件。\n",
    "\n",
    "根据我们的建议，验证模型的方法涉及会到创建一个json文件，例如包含模型预测的验证图像的标注的[这个文件](https://github.com/cocodataset/cocoapi/blob/master/results/captions_val2014_fakecap_results.json) 。然后，你可以编写自己的脚本或使用 [在线查找](https://github.com/tylin/coco-caption) 的脚本来计算模型的BLEU分数。你可以在 [本文](https://arxiv.org/pdf/1411.4555.pdf)第4.1节中阅读有关BLEU分数以及其他评估指标（如TEOR和Cider）的更多信息。有关如何使用注释文件的更多信息，请查看COCO数据集 [网站](http://cocodataset.org/#download) 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) TODO: Validate your model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
