{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算机视觉纳米学位项目\n",
    "\n",
    "## 实战项目：图像标注\n",
    "\n",
    "---\n",
    "\n",
    "在该 notebook 中，你要学习的是如何从 [COCO 数据集](http://cocodataset.org/#home) 中对数据进行加载和预处理。此外，你还要设计一个CNN-RNN模型，使其自动生成图像标注。\n",
    "\n",
    "请注意， **你对此notebook所做的任何修改，我们将不会对其进行评分**。  但是，你需要根据 **Step 3** 和 **Step 4** 中的说明，通过修改此项目的一部分，即**models.py**文件，从而实现你自己的CNN编码器和RNN解码器。我们将对你的**models.py** 文件进行评分。\n",
    "\n",
    "点击以下链接，即可进入此 notebook：\n",
    "- [Step 1](#step1): 了解数据加载器\n",
    "- [Step 2](#step2): 使用数据加载器获取批次\n",
    "- [Step 3](#step3): 使用CNN编码器进行实验\n",
    "- [Step 4](#step4): 实现RNN解码器\n",
    "\n",
    "<a id='step1'></a>\n",
    "## Step 1: 了解数据加载器\n",
    "\n",
    "我们已经编写了一个 [ 数据加载器](http://pytorch.org/docs/master/data.html#torch.utils.data.DataLoader) ，你可以使用它来批量加载COCO数据集。\n",
    "\n",
    "在下面的代码单元格中，你可以使用 **data_loader.py** 中的`get_loader` 函数对数据加载器初始化。\n",
    "\n",
    "> 在这个项目中，请不要修改 **data_loader.py** 文件，务必保留其原样。\n",
    "\n",
    " `get_loader` 函数将 **data_loader.py** 中可以用来探索的许多参数作为输入。现在，花一些时间在新窗口中打开 **data_loader.py**，并研究这些参数。大多数参数必须保留其默认值，只有以下参数的值允许被修改：\n",
    "1. **`transform`** -  [图像转换 ](http://pytorch.org/docs/master/torchvision/transforms.html) 具体规定了应该如何对图像进行预处理，并将它们转换为PyTorch张量，然后再将它们用作CNN编码器的输入。在这里，我们建议你保留`transform_train`中提供的转换方法。之后，你可以选择自己的图像转换方法，对COCO图像进行预处理。\n",
    "2. **`mode`** - `'train'`（用于批量加载训练数据）或 `'test'`（用于测试数据），二者中的一个。我们将分别说明数据加载器处于训练模式或测试模式的情况。参照该 notebook 中的说明进行操作时，请设置`mode='train'`.`'train'`，这样可以使数据加载器处于训练模式。\n",
    "3. **`batch_size`** - 它是用于确定批次的大小。训练你的模型时，它是指图像标注对的数量，用于在每个训练步骤中修改模型权重。\n",
    "4. **`vocab_threshold`** - 它是指在将单词用作词汇表的一部分之前，单词必须出现在训练图像标注中的总次数。在训练图像标注中出现少于`vocab_threshold` 的单词将被认为是未知单词。\n",
    "5. **`vocab_from_file`** -  它是指一个布尔运算（Boolean），用于决定是否从文件中加载词汇表。\n",
    "\n",
    "接下来，我们将更详细地描述`vocab_threshold` 和 `vocab_from_file`参数。现在，运行下面的代码单元格。要有耐心哦，可能需要几分钟才能运行！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.6/site-packages (3.2.5)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from nltk) (1.11.0)\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/opt/cocoapi/PythonAPI')\n",
    "from pycocotools.coco import COCO\n",
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from data_loader import get_loader\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform to pre-process the training images.\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "# Set the minimum word count threshold.\n",
    "vocab_threshold = 5\n",
    "\n",
    "# Specify the batch size.\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary successfully loaded from vocab.pkl file!\n",
      "loading annotations into memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/414113 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 434/414113 [00:00<01:35, 4334.34it/s]\u001b[A\n",
      "  0%|          | 823/414113 [00:00<01:38, 4190.51it/s]\u001b[A\n",
      "  0%|          | 1239/414113 [00:00<01:38, 4179.78it/s]\u001b[A\n",
      "  0%|          | 1660/414113 [00:00<01:38, 4187.61it/s]\u001b[A\n",
      "  0%|          | 2012/414113 [00:00<01:44, 3960.26it/s]\u001b[A\n",
      "  1%|          | 2407/414113 [00:00<01:44, 3956.41it/s]\u001b[A\n",
      "  1%|          | 2827/414113 [00:00<01:42, 4025.43it/s]\u001b[A\n",
      "  1%|          | 3261/414113 [00:00<01:39, 4113.09it/s]\u001b[A\n",
      "  1%|          | 3706/414113 [00:00<01:37, 4207.67it/s]\u001b[A\n",
      "  1%|          | 4148/414113 [00:01<01:36, 4266.94it/s]\u001b[A\n",
      "  1%|          | 4592/414113 [00:01<01:34, 4317.12it/s]\u001b[A\n",
      "  1%|          | 5043/414113 [00:01<01:33, 4373.03it/s]\u001b[A\n",
      "  1%|▏         | 5476/414113 [00:01<01:34, 4330.78it/s]\u001b[A\n",
      "  1%|▏         | 5927/414113 [00:01<01:33, 4381.82it/s]\u001b[A\n",
      "  2%|▏         | 6363/414113 [00:01<01:34, 4304.86it/s]\u001b[A\n",
      "  2%|▏         | 6804/414113 [00:01<01:33, 4333.61it/s]\u001b[A\n",
      "  2%|▏         | 7245/414113 [00:01<01:33, 4353.79it/s]\u001b[A\n",
      "  2%|▏         | 7690/414113 [00:01<01:32, 4379.82it/s]\u001b[A\n",
      "  2%|▏         | 8128/414113 [00:01<01:33, 4359.75it/s]\u001b[A\n",
      "  2%|▏         | 8573/414113 [00:02<01:32, 4385.74it/s]\u001b[A\n",
      "  2%|▏         | 9012/414113 [00:02<01:33, 4329.87it/s]\u001b[A\n",
      "  2%|▏         | 9454/414113 [00:02<01:32, 4355.09it/s]\u001b[A\n",
      "  2%|▏         | 9890/414113 [00:02<01:35, 4252.25it/s]\u001b[A\n",
      "  2%|▏         | 10316/414113 [00:02<01:35, 4248.93it/s]\u001b[A\n",
      "  3%|▎         | 10756/414113 [00:02<01:33, 4292.57it/s]\u001b[A\n",
      "  3%|▎         | 11186/414113 [00:02<01:34, 4250.35it/s]\u001b[A\n",
      "  3%|▎         | 11632/414113 [00:02<01:33, 4310.64it/s]\u001b[A\n",
      "  3%|▎         | 12072/414113 [00:02<01:32, 4335.47it/s]\u001b[A\n",
      "  3%|▎         | 12514/414113 [00:02<01:32, 4358.37it/s]\u001b[A\n",
      "  3%|▎         | 12951/414113 [00:03<01:32, 4347.80it/s]\u001b[A\n",
      "  3%|▎         | 13386/414113 [00:03<01:33, 4265.38it/s]\u001b[A\n",
      " 40%|███▉      | 164770/414113 [00:50<01:16, 3275.69it/s]\u001b[A\n",
      "  3%|▎         | 14252/414113 [00:03<01:33, 4284.10it/s]\u001b[A\n",
      "  4%|▎         | 14681/414113 [00:03<01:33, 4266.80it/s]\u001b[A\n",
      "  4%|▎         | 15108/414113 [00:03<01:33, 4254.73it/s]\u001b[A\n",
      "  4%|▍         | 15548/414113 [00:03<01:32, 4295.60it/s]\u001b[A\n",
      "  4%|▍         | 15981/414113 [00:03<01:32, 4304.27it/s]\u001b[A\n",
      "  4%|▍         | 16429/414113 [00:03<01:31, 4353.82it/s]\u001b[A\n",
      "  4%|▍         | 16874/414113 [00:03<01:30, 4380.63it/s]\u001b[A\n",
      "  4%|▍         | 17313/414113 [00:04<01:31, 4348.95it/s]\u001b[A\n",
      "  4%|▍         | 17749/414113 [00:04<01:31, 4323.58it/s]\u001b[A\n",
      "  4%|▍         | 18182/414113 [00:04<01:32, 4278.34it/s]\u001b[A\n",
      "  4%|▍         | 18611/414113 [00:04<01:34, 4199.78it/s]\u001b[A\n",
      "  5%|▍         | 19032/414113 [00:04<01:34, 4182.11it/s]\u001b[A\n",
      "  5%|▍         | 19478/414113 [00:04<01:32, 4259.83it/s]\u001b[A\n",
      "  5%|▍         | 19927/414113 [00:04<01:31, 4324.76it/s]\u001b[A\n",
      "  5%|▍         | 20361/414113 [00:04<01:31, 4306.51it/s]\u001b[A\n",
      "  5%|▌         | 20801/414113 [00:04<01:30, 4332.53it/s]\u001b[A\n",
      "  5%|▌         | 21237/414113 [00:04<01:30, 4340.51it/s]\u001b[A\n",
      "  5%|▌         | 21685/414113 [00:05<01:29, 4379.10it/s]\u001b[A\n",
      "  5%|▌         | 22133/414113 [00:05<01:28, 4406.06it/s]\u001b[A\n",
      "  5%|▌         | 22586/414113 [00:05<01:28, 4442.32it/s]\u001b[A\n",
      "  6%|▌         | 23043/414113 [00:05<01:27, 4478.21it/s]\u001b[A\n",
      "  6%|▌         | 23492/414113 [00:05<01:28, 4415.69it/s]\u001b[A\n",
      "  6%|▌         | 23934/414113 [00:05<01:28, 4389.80it/s]\u001b[A\n",
      "  6%|▌         | 24390/414113 [00:05<01:27, 4437.28it/s]\u001b[A\n",
      "  6%|▌         | 24835/414113 [00:05<01:28, 4407.70it/s]\u001b[A\n",
      "  6%|▌         | 25277/414113 [00:05<01:28, 4393.47it/s]\u001b[A\n",
      "  6%|▌         | 25726/414113 [00:05<01:27, 4420.79it/s]\u001b[A\n",
      "  6%|▋         | 26169/414113 [00:06<01:28, 4373.05it/s]\u001b[A\n",
      "  6%|▋         | 26614/414113 [00:06<01:28, 4395.03it/s]\u001b[A\n",
      "  7%|▋         | 27054/414113 [00:06<01:29, 4345.51it/s]\u001b[A\n",
      "  7%|▋         | 27489/414113 [00:06<01:29, 4299.25it/s]\u001b[A\n",
      "  7%|▋         | 27927/414113 [00:06<01:29, 4321.52it/s]\u001b[A\n",
      "  7%|▋         | 28383/414113 [00:06<01:27, 4390.25it/s]\u001b[A\n",
      "  7%|▋         | 28834/414113 [00:06<01:27, 4425.05it/s]\u001b[A\n",
      "  7%|▋         | 29277/414113 [00:06<01:28, 4372.38it/s]\u001b[A\n",
      "  7%|▋         | 29715/414113 [00:06<01:28, 4345.74it/s]\u001b[A\n",
      "  7%|▋         | 30168/414113 [00:06<01:27, 4397.09it/s]\u001b[A\n",
      "  7%|▋         | 30609/414113 [00:07<01:27, 4370.87it/s]\u001b[A\n",
      "  8%|▊         | 31069/414113 [00:07<01:26, 4436.26it/s]\u001b[A\n",
      "  8%|▊         | 31514/414113 [00:07<01:26, 4427.63it/s]\u001b[A\n",
      "  8%|▊         | 31958/414113 [00:07<01:26, 4395.76it/s]\u001b[A\n",
      "  8%|▊         | 32412/414113 [00:07<01:26, 4437.26it/s]\u001b[A\n",
      "  8%|▊         | 32857/414113 [00:07<01:26, 4416.44it/s]\u001b[A\n",
      "  8%|▊         | 33299/414113 [00:07<01:26, 4411.82it/s]\u001b[A\n",
      "  8%|▊         | 33752/414113 [00:07<01:25, 4445.43it/s]\u001b[A\n",
      "  8%|▊         | 34197/414113 [00:07<01:26, 4404.71it/s]\u001b[A\n",
      "  8%|▊         | 34639/414113 [00:08<01:26, 4406.99it/s]\u001b[A\n",
      "  8%|▊         | 35080/414113 [00:08<01:28, 4306.35it/s]\u001b[A\n",
      "  9%|▊         | 35512/414113 [00:08<01:31, 4115.34it/s]\u001b[A\n",
      "  9%|▊         | 35950/414113 [00:08<01:30, 4190.98it/s]\u001b[A\n",
      "  9%|▉         | 36377/414113 [00:08<01:29, 4212.06it/s]\u001b[A\n",
      "  9%|▉         | 36830/414113 [00:08<01:27, 4300.27it/s]\u001b[A\n",
      "  9%|▉         | 37279/414113 [00:08<01:26, 4355.31it/s]\u001b[A\n",
      "  9%|▉         | 37737/414113 [00:08<01:25, 4417.89it/s]\u001b[A\n",
      "  9%|▉         | 38189/414113 [00:08<01:24, 4446.78it/s]\u001b[A\n",
      "  9%|▉         | 38635/414113 [00:08<01:24, 4432.08it/s]\u001b[A\n",
      "  9%|▉         | 39079/414113 [00:09<01:24, 4417.19it/s]\u001b[A\n",
      " 10%|▉         | 39522/414113 [00:09<01:24, 4410.04it/s]\u001b[A\n",
      " 10%|▉         | 39965/414113 [00:09<01:24, 4415.41it/s]\u001b[A\n",
      " 10%|▉         | 40410/414113 [00:09<01:24, 4423.17it/s]\u001b[A\n",
      " 10%|▉         | 40853/414113 [00:09<01:24, 4412.54it/s]\u001b[A\n",
      " 10%|▉         | 41295/414113 [00:09<01:25, 4352.04it/s]\u001b[A\n",
      " 10%|█         | 41734/414113 [00:09<01:25, 4362.50it/s]\u001b[A\n",
      " 10%|█         | 42180/414113 [00:09<01:24, 4391.05it/s]\u001b[A\n",
      " 10%|█         | 42627/414113 [00:09<01:24, 4412.00it/s]\u001b[A\n",
      " 10%|█         | 43078/414113 [00:09<01:23, 4439.39it/s]\u001b[A\n",
      " 11%|█         | 43523/414113 [00:10<01:24, 4381.20it/s]\u001b[A\n",
      " 11%|█         | 43962/414113 [00:10<01:24, 4369.34it/s]\u001b[A\n",
      " 11%|█         | 44410/414113 [00:10<01:24, 4400.16it/s]\u001b[A\n",
      " 11%|█         | 44851/414113 [00:10<01:24, 4393.09it/s]\u001b[A\n",
      " 11%|█         | 45300/414113 [00:10<01:23, 4421.35it/s]\u001b[A\n",
      " 11%|█         | 45743/414113 [00:10<01:23, 4418.78it/s]\u001b[A\n",
      " 11%|█         | 46198/414113 [00:10<01:22, 4454.60it/s]\u001b[A\n",
      " 11%|█▏        | 46650/414113 [00:10<01:22, 4473.26it/s]\u001b[A\n",
      " 11%|█▏        | 47098/414113 [00:10<01:22, 4458.01it/s]\u001b[A\n",
      " 11%|█▏        | 47550/414113 [00:10<01:21, 4474.27it/s]\u001b[A\n",
      " 12%|█▏        | 47998/414113 [00:11<01:22, 4450.95it/s]\u001b[A\n",
      " 12%|█▏        | 48444/414113 [00:11<01:23, 4389.95it/s]\u001b[A\n",
      " 12%|█▏        | 48884/414113 [00:11<01:24, 4321.66it/s]\u001b[A\n",
      " 12%|█▏        | 49317/414113 [00:11<01:24, 4293.44it/s]\u001b[A\n",
      " 12%|█▏        | 49747/414113 [00:11<01:25, 4241.60it/s]\u001b[A\n",
      " 12%|█▏        | 50172/414113 [00:11<01:25, 4236.17it/s]\u001b[A\n",
      " 12%|█▏        | 50596/414113 [00:11<01:25, 4234.32it/s]\u001b[A\n",
      " 12%|█▏        | 51020/414113 [00:11<01:30, 4018.45it/s]\u001b[A\n",
      " 12%|█▏        | 51425/414113 [00:11<01:30, 4022.85it/s]\u001b[A\n",
      " 13%|█▎        | 51850/414113 [00:11<01:28, 4087.45it/s]\u001b[A\n",
      " 13%|█▎        | 52263/414113 [00:12<01:28, 4096.82it/s]\u001b[A\n",
      " 13%|█▎        | 52674/414113 [00:12<02:18, 2614.54it/s]\u001b[A\n",
      " 13%|█▎        | 53125/414113 [00:12<02:00, 2990.64it/s]\u001b[A\n",
      " 13%|█▎        | 53577/414113 [00:12<01:48, 3328.28it/s]\u001b[A\n",
      " 13%|█▎        | 54007/414113 [00:12<01:40, 3569.02it/s]\u001b[A\n",
      " 13%|█▎        | 54432/414113 [00:12<01:35, 3747.15it/s]\u001b[A\n",
      " 13%|█▎        | 54862/414113 [00:12<01:32, 3895.74it/s]\u001b[A\n",
      " 13%|█▎        | 55319/414113 [00:12<01:28, 4074.38it/s]\u001b[A\n",
      " 13%|█▎        | 55773/414113 [00:13<01:25, 4201.87it/s]\u001b[A\n",
      " 14%|█▎        | 56215/414113 [00:13<01:23, 4263.64it/s]\u001b[A\n",
      " 14%|█▎        | 56666/414113 [00:13<01:22, 4333.37it/s]\u001b[A\n",
      " 14%|█▍        | 57114/414113 [00:13<01:21, 4373.86it/s]\u001b[A\n",
      " 14%|█▍        | 57573/414113 [00:13<01:20, 4435.36it/s]\u001b[A\n",
      " 14%|█▍        | 58021/414113 [00:13<01:20, 4431.25it/s]\u001b[A\n",
      " 14%|█▍        | 58468/414113 [00:13<01:20, 4417.96it/s]\u001b[A\n",
      " 14%|█▍        | 58920/414113 [00:13<01:19, 4447.00it/s]\u001b[A\n",
      " 14%|█▍        | 59370/414113 [00:13<01:19, 4460.07it/s]\u001b[A\n",
      " 14%|█▍        | 59818/414113 [00:13<01:19, 4451.42it/s]\u001b[A\n",
      " 15%|█▍        | 60264/414113 [00:14<01:19, 4432.10it/s]\u001b[A\n",
      " 15%|█▍        | 60710/414113 [00:14<01:19, 4440.41it/s]\u001b[A\n",
      " 15%|█▍        | 61155/414113 [00:14<01:20, 4396.36it/s]\u001b[A\n",
      " 15%|█▍        | 61611/414113 [00:14<01:19, 4442.91it/s]\u001b[A\n",
      " 15%|█▍        | 62059/414113 [00:14<01:19, 4451.13it/s]\u001b[A\n",
      " 15%|█▌        | 62509/414113 [00:14<01:18, 4462.59it/s]\u001b[A\n",
      " 15%|█▌        | 62956/414113 [00:14<01:20, 4379.88it/s]\u001b[A\n",
      " 15%|█▌        | 63395/414113 [00:14<01:20, 4334.86it/s]\u001b[A\n",
      " 15%|█▌        | 63829/414113 [00:14<01:22, 4271.54it/s]\u001b[A\n",
      " 16%|█▌        | 64283/414113 [00:14<01:20, 4347.85it/s]\u001b[A\n",
      " 16%|█▌        | 64731/414113 [00:15<01:19, 4383.23it/s]\u001b[A\n",
      " 16%|█▌        | 65170/414113 [00:15<01:19, 4383.98it/s]\u001b[A\n",
      " 16%|█▌        | 65614/414113 [00:15<01:19, 4398.01it/s]\u001b[A\n",
      " 16%|█▌        | 66055/414113 [00:15<01:22, 4218.43it/s]\u001b[A\n",
      " 16%|█▌        | 66514/414113 [00:15<01:20, 4322.08it/s]\u001b[A\n",
      " 16%|█▌        | 66960/414113 [00:15<01:19, 4361.88it/s]\u001b[A\n",
      " 16%|█▋        | 67407/414113 [00:15<01:18, 4393.70it/s]\u001b[A\n",
      " 16%|█▋        | 67862/414113 [00:15<01:18, 4439.04it/s]\u001b[A\n",
      " 16%|█▋        | 68307/414113 [00:15<01:18, 4400.49it/s]\u001b[A\n",
      " 17%|█▋        | 68748/414113 [00:16<01:19, 4324.43it/s]\u001b[A\n",
      " 17%|█▋        | 69185/414113 [00:16<01:19, 4321.83it/s]\u001b[A\n",
      " 17%|█▋        | 69630/414113 [00:16<01:19, 4359.25it/s]\u001b[A\n",
      " 17%|█▋        | 70068/414113 [00:16<01:18, 4364.12it/s]\u001b[A\n",
      " 17%|█▋        | 70505/414113 [00:16<01:18, 4360.91it/s]\u001b[A\n",
      " 17%|█▋        | 70958/414113 [00:16<01:17, 4409.42it/s]\u001b[A\n",
      " 17%|█▋        | 71412/414113 [00:16<01:17, 4445.93it/s]\u001b[A\n",
      " 17%|█▋        | 71861/414113 [00:16<01:16, 4458.28it/s]\u001b[A\n",
      " 17%|█▋        | 72308/414113 [00:16<01:17, 4405.23it/s]\u001b[A\n",
      " 18%|█▊        | 72749/414113 [00:16<01:19, 4313.30it/s]\u001b[A\n",
      " 18%|█▊        | 73181/414113 [00:17<01:19, 4279.58it/s]\u001b[A\n",
      " 18%|█▊        | 73610/414113 [00:17<01:19, 4262.68it/s]\u001b[A\n",
      " 18%|█▊        | 74037/414113 [00:17<01:20, 4238.68it/s]\u001b[A\n",
      " 18%|█▊        | 74462/414113 [00:17<01:20, 4218.15it/s]\u001b[A\n",
      " 18%|█▊        | 74885/414113 [00:17<01:21, 4184.47it/s]\u001b[A\n",
      " 18%|█▊        | 75304/414113 [00:17<01:21, 4142.72it/s]\u001b[A\n",
      " 18%|█▊        | 75719/414113 [00:17<01:21, 4139.46it/s]\u001b[A\n",
      " 18%|█▊        | 76136/414113 [00:17<01:21, 4145.92it/s]\u001b[A\n",
      " 18%|█▊        | 76551/414113 [00:17<01:21, 4131.93it/s]\u001b[A\n",
      " 19%|█▊        | 76984/414113 [00:17<01:20, 4188.29it/s]\u001b[A\n",
      " 19%|█▊        | 77430/414113 [00:18<01:18, 4263.83it/s]\u001b[A\n",
      " 19%|█▉        | 77857/414113 [00:18<01:20, 4174.53it/s]\u001b[A\n",
      " 19%|█▉        | 78304/414113 [00:18<01:18, 4256.87it/s]\u001b[A\n",
      " 19%|█▉        | 78731/414113 [00:18<01:19, 4198.30it/s]\u001b[A\n",
      " 19%|█▉        | 79168/414113 [00:18<01:18, 4247.44it/s]\u001b[A\n",
      " 19%|█▉        | 79628/414113 [00:18<01:16, 4346.48it/s]\u001b[A\n",
      " 19%|█▉        | 80085/414113 [00:18<01:15, 4410.91it/s]\u001b[A\n",
      " 19%|█▉        | 80544/414113 [00:18<01:14, 4461.57it/s]\u001b[A\n",
      " 20%|█▉        | 80997/414113 [00:18<01:14, 4480.52it/s]\u001b[A\n",
      " 20%|█▉        | 81446/414113 [00:18<01:14, 4479.79it/s]\u001b[A\n",
      " 20%|█▉        | 81898/414113 [00:19<01:13, 4490.54it/s]\u001b[A\n",
      " 20%|█▉        | 82348/414113 [00:19<01:13, 4492.01it/s]\u001b[A\n",
      " 20%|█▉        | 82804/414113 [00:19<01:13, 4509.66it/s]\u001b[A\n",
      " 20%|██        | 83271/414113 [00:19<01:12, 4555.04it/s]\u001b[A\n",
      " 20%|██        | 83727/414113 [00:19<01:13, 4501.09it/s]\u001b[A\n",
      " 20%|██        | 84178/414113 [00:19<01:13, 4491.05it/s]\u001b[A\n",
      " 20%|██        | 84632/414113 [00:19<01:13, 4503.88it/s]\u001b[A\n",
      " 21%|██        | 85083/414113 [00:19<01:13, 4476.90it/s]\u001b[A\n",
      " 21%|██        | 85531/414113 [00:19<01:13, 4471.19it/s]\u001b[A\n",
      " 21%|██        | 85979/414113 [00:19<01:13, 4461.42it/s]\u001b[A\n",
      " 21%|██        | 86431/414113 [00:20<01:13, 4476.68it/s]\u001b[A\n",
      " 21%|██        | 86889/414113 [00:20<01:12, 4504.73it/s]\u001b[A\n",
      " 21%|██        | 87340/414113 [00:20<01:12, 4484.64it/s]\u001b[A\n",
      " 21%|██        | 87791/414113 [00:20<01:12, 4491.96it/s]\u001b[A\n",
      " 21%|██▏       | 88245/414113 [00:20<01:12, 4504.23it/s]\u001b[A\n",
      " 21%|██▏       | 88696/414113 [00:20<01:12, 4487.79it/s]\u001b[A\n",
      " 22%|██▏       | 89145/414113 [00:20<01:12, 4488.18it/s]\u001b[A\n",
      " 22%|██▏       | 89602/414113 [00:20<01:11, 4511.78it/s]\u001b[A\n",
      " 22%|██▏       | 90055/414113 [00:20<01:11, 4516.48it/s]\u001b[A\n",
      " 22%|██▏       | 90507/414113 [00:20<01:11, 4511.32it/s]\u001b[A\n",
      " 22%|██▏       | 90959/414113 [00:21<01:12, 4480.77it/s]\u001b[A\n",
      " 22%|██▏       | 91408/414113 [00:21<01:13, 4382.40it/s]\u001b[A\n",
      " 22%|██▏       | 91847/414113 [00:21<01:14, 4343.56it/s]\u001b[A\n",
      " 22%|██▏       | 92282/414113 [00:21<01:14, 4324.14it/s]\u001b[A\n",
      " 22%|██▏       | 92715/414113 [00:21<01:14, 4298.32it/s]\u001b[A\n",
      " 22%|██▏       | 93167/414113 [00:21<01:13, 4360.37it/s]\u001b[A\n",
      " 23%|██▎       | 93604/414113 [00:21<01:13, 4347.48it/s]\u001b[A\n",
      " 23%|██▎       | 94060/414113 [00:21<01:12, 4408.06it/s]\u001b[A\n",
      " 23%|██▎       | 94502/414113 [00:21<01:19, 4012.37it/s]\u001b[A\n",
      " 23%|██▎       | 94931/414113 [00:22<01:18, 4090.02it/s]\u001b[A\n",
      " 23%|██▎       | 95364/414113 [00:22<01:16, 4158.68it/s]\u001b[A\n",
      " 23%|██▎       | 95805/414113 [00:22<01:15, 4228.30it/s]\u001b[A\n",
      " 23%|██▎       | 96243/414113 [00:22<01:14, 4272.56it/s]\u001b[A\n",
      " 23%|██▎       | 96695/414113 [00:22<01:13, 4343.69it/s]\u001b[A\n",
      " 23%|██▎       | 97132/414113 [00:22<01:12, 4350.73it/s]\u001b[A\n",
      " 24%|██▎       | 97579/414113 [00:22<01:12, 4384.86it/s]\u001b[A\n",
      " 24%|██▎       | 98022/414113 [00:22<01:11, 4397.66it/s]\u001b[A\n",
      " 24%|██▍       | 98464/414113 [00:22<01:11, 4402.69it/s]\u001b[A\n",
      " 24%|██▍       | 98919/414113 [00:22<01:10, 4443.79it/s]\u001b[A\n",
      " 24%|██▍       | 99369/414113 [00:23<01:10, 4458.90it/s]\u001b[A\n",
      " 24%|██▍       | 99817/414113 [00:23<01:10, 4463.37it/s]\u001b[A\n",
      " 24%|██▍       | 100265/414113 [00:23<01:10, 4467.85it/s]\u001b[A\n",
      " 24%|██▍       | 100712/414113 [00:23<01:10, 4438.20it/s]\u001b[A\n",
      " 24%|██▍       | 101156/414113 [00:23<01:11, 4348.61it/s]\u001b[A\n",
      " 25%|██▍       | 101592/414113 [00:23<01:13, 4264.15it/s]\u001b[A\n",
      " 25%|██▍       | 102020/414113 [00:23<01:13, 4238.90it/s]\u001b[A\n",
      " 25%|██▍       | 102445/414113 [00:23<01:13, 4225.00it/s]\u001b[A\n",
      " 25%|██▍       | 102868/414113 [00:23<01:14, 4203.56it/s]\u001b[A\n",
      " 25%|██▍       | 103294/414113 [00:23<01:13, 4219.56it/s]\u001b[A\n",
      " 25%|██▌       | 103717/414113 [00:24<01:13, 4204.86it/s]\u001b[A\n",
      " 25%|██▌       | 104138/414113 [00:24<01:14, 4141.60it/s]\u001b[A\n",
      " 25%|██▌       | 104553/414113 [00:24<01:15, 4112.80it/s]\u001b[A\n",
      " 25%|██▌       | 104972/414113 [00:24<01:14, 4133.05it/s]\u001b[A\n",
      " 25%|██▌       | 105418/414113 [00:24<01:13, 4224.97it/s]\u001b[A\n",
      " 26%|██▌       | 105871/414113 [00:24<01:11, 4311.33it/s]\u001b[A\n",
      " 26%|██▌       | 106323/414113 [00:24<01:10, 4370.43it/s]\u001b[A\n",
      " 26%|██▌       | 106769/414113 [00:24<01:09, 4394.77it/s]\u001b[A\n",
      " 26%|██▌       | 107210/414113 [00:24<01:10, 4381.10it/s]\u001b[A\n",
      " 26%|██▌       | 107659/414113 [00:24<01:09, 4410.59it/s]\u001b[A\n",
      " 26%|██▌       | 108102/414113 [00:25<01:09, 4415.83it/s]\u001b[A\n",
      " 26%|██▌       | 108553/414113 [00:25<01:08, 4441.18it/s]\u001b[A\n",
      " 26%|██▋       | 108998/414113 [00:25<01:10, 4358.40it/s]\u001b[A\n",
      " 26%|██▋       | 109435/414113 [00:25<01:10, 4309.84it/s]\u001b[A\n",
      " 27%|██▋       | 109867/414113 [00:25<01:11, 4272.81it/s]\u001b[A\n",
      " 27%|██▋       | 110295/414113 [00:25<01:11, 4266.82it/s]\u001b[A\n",
      " 27%|██▋       | 110752/414113 [00:25<01:09, 4350.94it/s]\u001b[A\n",
      " 27%|██▋       | 111197/414113 [00:25<01:09, 4378.66it/s]\u001b[A\n",
      " 27%|██▋       | 111639/414113 [00:25<01:08, 4389.02it/s]\u001b[A\n",
      " 27%|██▋       | 112079/414113 [00:25<01:08, 4387.55it/s]\u001b[A\n",
      " 27%|██▋       | 112518/414113 [00:26<01:09, 4318.59it/s]\u001b[A\n",
      " 27%|██▋       | 112951/414113 [00:26<01:10, 4290.23it/s]\u001b[A\n",
      " 27%|██▋       | 113382/414113 [00:26<01:10, 4294.57it/s]\u001b[A\n",
      " 27%|██▋       | 113830/414113 [00:26<01:09, 4347.33it/s]\u001b[A\n",
      " 28%|██▊       | 114276/414113 [00:26<01:08, 4380.52it/s]\u001b[A\n",
      " 28%|██▊       | 114726/414113 [00:26<01:07, 4414.94it/s]\u001b[A\n",
      " 28%|██▊       | 115178/414113 [00:26<01:07, 4443.58it/s]\u001b[A\n",
      " 28%|██▊       | 115629/414113 [00:26<01:06, 4462.92it/s]\u001b[A\n",
      " 28%|██▊       | 116076/414113 [00:26<01:07, 4446.10it/s]\u001b[A\n",
      " 28%|██▊       | 116521/414113 [00:26<01:07, 4434.32it/s]\u001b[A\n",
      " 28%|██▊       | 116965/414113 [00:27<01:08, 4335.78it/s]\u001b[A\n",
      " 28%|██▊       | 117400/414113 [00:27<01:09, 4292.79it/s]\u001b[A\n",
      " 28%|██▊       | 117830/414113 [00:27<01:09, 4249.74it/s]\u001b[A\n",
      " 29%|██▊       | 118256/414113 [00:27<01:13, 4016.94it/s]\u001b[A\n",
      " 29%|██▊       | 118676/414113 [00:27<01:12, 4069.22it/s]\u001b[A\n",
      " 29%|██▉       | 119100/414113 [00:27<01:11, 4118.43it/s]\u001b[A\n",
      " 29%|██▉       | 119546/414113 [00:27<01:09, 4212.17it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 120000/414113 [00:27<01:08, 4302.85it/s]\u001b[A\n",
      " 29%|██▉       | 120439/414113 [00:27<01:07, 4327.53it/s]\u001b[A\n",
      " 29%|██▉       | 120885/414113 [00:28<01:07, 4365.46it/s]\u001b[A\n",
      " 29%|██▉       | 121333/414113 [00:28<01:06, 4396.99it/s]\u001b[A\n",
      " 29%|██▉       | 121774/414113 [00:28<01:06, 4399.45it/s]\u001b[A\n",
      " 30%|██▉       | 122215/414113 [00:28<01:06, 4389.97it/s]\u001b[A\n",
      " 30%|██▉       | 122655/414113 [00:28<01:06, 4355.86it/s]\u001b[A\n",
      " 30%|██▉       | 123091/414113 [00:28<01:07, 4305.54it/s]\u001b[A\n",
      " 30%|██▉       | 123523/414113 [00:28<01:07, 4309.40it/s]\u001b[A\n",
      " 30%|██▉       | 123955/414113 [00:28<01:07, 4287.87it/s]\u001b[A\n",
      " 30%|███       | 124398/414113 [00:28<01:06, 4328.69it/s]\u001b[A\n",
      " 30%|███       | 124842/414113 [00:28<01:06, 4358.90it/s]\u001b[A\n",
      " 30%|███       | 125287/414113 [00:29<01:05, 4385.60it/s]\u001b[A\n",
      " 30%|███       | 125738/414113 [00:29<01:05, 4420.62it/s]\u001b[A\n",
      " 30%|███       | 126195/414113 [00:29<01:04, 4463.48it/s]\u001b[A\n",
      " 31%|███       | 126642/414113 [00:29<01:04, 4456.54it/s]\u001b[A\n",
      " 31%|███       | 127101/414113 [00:29<01:03, 4494.01it/s]\u001b[A\n",
      " 31%|███       | 127551/414113 [00:29<01:03, 4486.84it/s]\u001b[A\n",
      " 31%|███       | 128001/414113 [00:29<01:03, 4488.66it/s]\u001b[A\n",
      " 31%|███       | 128457/414113 [00:29<01:03, 4508.82it/s]\u001b[A\n",
      " 31%|███       | 128908/414113 [00:29<01:03, 4488.90it/s]\u001b[A\n",
      " 31%|███       | 129357/414113 [00:29<01:03, 4486.10it/s]\u001b[A\n",
      " 31%|███▏      | 129817/414113 [00:30<01:02, 4518.91it/s]\u001b[A\n",
      " 31%|███▏      | 130269/414113 [00:30<01:03, 4501.33it/s]\u001b[A\n",
      " 32%|███▏      | 130723/414113 [00:30<01:02, 4510.00it/s]\u001b[A\n",
      " 32%|███▏      | 131183/414113 [00:30<01:02, 4534.84it/s]\u001b[A\n",
      " 32%|███▏      | 131637/414113 [00:30<01:03, 4420.30it/s]\u001b[A\n",
      " 32%|███▏      | 132080/414113 [00:30<01:04, 4359.21it/s]\u001b[A\n",
      " 32%|███▏      | 132517/414113 [00:30<01:05, 4300.53it/s]\u001b[A\n",
      " 32%|███▏      | 132948/414113 [00:30<01:05, 4278.74it/s]\u001b[A\n",
      " 32%|███▏      | 133377/414113 [00:30<01:06, 4236.13it/s]\u001b[A\n",
      " 32%|███▏      | 133803/414113 [00:30<01:06, 4241.06it/s]\u001b[A\n",
      " 32%|███▏      | 134228/414113 [00:31<01:06, 4180.72it/s]\u001b[A\n",
      " 33%|███▎      | 134647/414113 [00:31<01:07, 4161.01it/s]\u001b[A\n",
      " 33%|███▎      | 135079/414113 [00:31<01:06, 4205.67it/s]\u001b[A\n",
      " 33%|███▎      | 135500/414113 [00:31<01:06, 4174.20it/s]\u001b[A\n",
      " 33%|███▎      | 135934/414113 [00:31<01:05, 4221.30it/s]\u001b[A\n",
      " 33%|███▎      | 136384/414113 [00:31<01:04, 4300.99it/s]\u001b[A\n",
      " 33%|███▎      | 136835/414113 [00:31<01:03, 4359.88it/s]\u001b[A\n",
      " 33%|███▎      | 137272/414113 [00:31<01:03, 4346.87it/s]\u001b[A\n",
      " 33%|███▎      | 137710/414113 [00:31<01:03, 4356.50it/s]\u001b[A\n",
      " 33%|███▎      | 138156/414113 [00:31<01:02, 4386.86it/s]\u001b[A\n",
      " 33%|███▎      | 138595/414113 [00:32<01:03, 4337.22it/s]\u001b[A\n",
      " 34%|███▎      | 139030/414113 [00:32<01:03, 4307.95it/s]\u001b[A\n",
      " 34%|███▎      | 139462/414113 [00:32<01:03, 4302.22it/s]\u001b[A\n",
      " 34%|███▍      | 139910/414113 [00:32<01:03, 4351.03it/s]\u001b[A\n",
      " 34%|███▍      | 140356/414113 [00:32<01:02, 4382.59it/s]\u001b[A\n",
      " 34%|███▍      | 140801/414113 [00:32<01:02, 4401.98it/s]\u001b[A\n",
      " 34%|███▍      | 141246/414113 [00:32<01:01, 4413.52it/s]\u001b[A\n",
      " 34%|███▍      | 141688/414113 [00:32<01:01, 4396.55it/s]\u001b[A\n",
      " 34%|███▍      | 142146/414113 [00:32<01:01, 4448.91it/s]\u001b[A\n",
      " 34%|███▍      | 142592/414113 [00:32<01:01, 4440.61it/s]\u001b[A\n",
      " 35%|███▍      | 143037/414113 [00:33<01:01, 4394.49it/s]\u001b[A\n",
      " 35%|███▍      | 143477/414113 [00:33<01:02, 4323.49it/s]\u001b[A\n",
      " 35%|███▍      | 143910/414113 [00:33<01:02, 4290.23it/s]\u001b[A\n",
      " 35%|███▍      | 144340/414113 [00:33<01:03, 4246.53it/s]\u001b[A\n",
      " 35%|███▍      | 144765/414113 [00:33<01:03, 4230.29it/s]\u001b[A\n",
      " 35%|███▌      | 145189/414113 [00:33<01:47, 2507.64it/s]\u001b[A\n",
      " 35%|███▌      | 145617/414113 [00:33<01:33, 2863.29it/s]\u001b[A\n",
      " 35%|███▌      | 146031/414113 [00:34<01:25, 3153.71it/s]\u001b[A\n",
      " 35%|███▌      | 146465/414113 [00:34<01:17, 3433.50it/s]\u001b[A\n",
      " 35%|███▌      | 146882/414113 [00:34<01:13, 3624.10it/s]\u001b[A\n",
      " 36%|███▌      | 147312/414113 [00:34<01:10, 3802.47it/s]\u001b[A\n",
      " 36%|███▌      | 147746/414113 [00:34<01:07, 3948.64it/s]\u001b[A\n",
      " 36%|███▌      | 148199/414113 [00:34<01:04, 4106.70it/s]\u001b[A\n",
      " 36%|███▌      | 148659/414113 [00:34<01:02, 4242.95it/s]\u001b[A\n",
      " 36%|███▌      | 149109/414113 [00:34<01:01, 4316.12it/s]\u001b[A\n",
      " 36%|███▌      | 149558/414113 [00:34<01:00, 4366.17it/s]\u001b[A\n",
      " 36%|███▌      | 150002/414113 [00:34<01:00, 4375.50it/s]\u001b[A\n",
      " 36%|███▋      | 150445/414113 [00:35<01:00, 4373.42it/s]\u001b[A\n",
      " 36%|███▋      | 150899/414113 [00:35<00:59, 4421.74it/s]\u001b[A\n",
      " 37%|███▋      | 151344/414113 [00:35<00:59, 4405.72it/s]\u001b[A\n",
      " 37%|███▋      | 151799/414113 [00:35<00:58, 4447.04it/s]\u001b[A\n",
      " 37%|███▋      | 152246/414113 [00:35<00:58, 4448.86it/s]\u001b[A\n",
      " 37%|███▋      | 152705/414113 [00:35<00:58, 4487.41it/s]\u001b[A\n",
      " 37%|███▋      | 153163/414113 [00:35<00:57, 4513.63it/s]\u001b[A\n",
      " 37%|███▋      | 153625/414113 [00:35<00:57, 4542.51it/s]\u001b[A\n",
      " 37%|███▋      | 154080/414113 [00:35<00:57, 4534.13it/s]\u001b[A\n",
      " 37%|███▋      | 154541/414113 [00:35<00:56, 4555.95it/s]\u001b[A\n",
      " 37%|███▋      | 154997/414113 [00:36<00:58, 4428.96it/s]\u001b[A\n",
      " 38%|███▊      | 155447/414113 [00:36<00:58, 4449.19it/s]\u001b[A\n",
      " 38%|███▊      | 155893/414113 [00:36<00:58, 4384.06it/s]\u001b[A\n",
      " 38%|███▊      | 156341/414113 [00:36<00:58, 4411.08it/s]\u001b[A\n",
      " 38%|███▊      | 156794/414113 [00:36<00:57, 4443.19it/s]\u001b[A\n",
      " 38%|███▊      | 157239/414113 [00:36<00:57, 4438.95it/s]\u001b[A\n",
      " 38%|███▊      | 157693/414113 [00:36<00:57, 4468.71it/s]\u001b[A\n",
      " 38%|███▊      | 158141/414113 [00:36<00:57, 4442.97it/s]\u001b[A\n",
      " 38%|███▊      | 158586/414113 [00:36<00:58, 4384.33it/s]\u001b[A\n",
      " 38%|███▊      | 159027/414113 [00:36<00:58, 4390.30it/s]\u001b[A\n",
      " 39%|███▊      | 159481/414113 [00:37<00:57, 4433.22it/s]\u001b[A\n",
      " 39%|███▊      | 159954/414113 [00:37<00:56, 4516.21it/s]\u001b[A\n",
      " 39%|███▊      | 160408/414113 [00:37<00:56, 4521.14it/s]\u001b[A\n",
      " 39%|███▉      | 160865/414113 [00:37<00:55, 4533.03it/s]\u001b[A\n",
      " 39%|███▉      | 161329/414113 [00:37<00:55, 4562.92it/s]\u001b[A\n",
      " 39%|███▉      | 161786/414113 [00:37<00:55, 4521.87it/s]\u001b[A\n",
      " 39%|███▉      | 162247/414113 [00:37<00:55, 4546.66it/s]\u001b[A\n",
      " 39%|███▉      | 162714/414113 [00:37<00:54, 4581.76it/s]\u001b[A\n",
      " 39%|███▉      | 163173/414113 [00:37<00:54, 4566.53it/s]\u001b[A\n",
      " 40%|███▉      | 163630/414113 [00:37<00:54, 4560.25it/s]\u001b[A\n",
      " 40%|███▉      | 164087/414113 [00:38<00:57, 4384.13it/s]\u001b[A\n",
      " 40%|███▉      | 164533/414113 [00:38<00:56, 4403.87it/s]\u001b[A\n",
      " 40%|███▉      | 164975/414113 [00:38<00:57, 4369.67it/s]\u001b[A\n",
      " 40%|███▉      | 165416/414113 [00:38<00:56, 4380.74it/s]\u001b[A\n",
      " 40%|████      | 165863/414113 [00:38<00:56, 4405.79it/s]\u001b[A\n",
      " 40%|████      | 166305/414113 [00:38<00:56, 4382.61it/s]\u001b[A\n",
      " 40%|████      | 166744/414113 [00:38<00:56, 4374.59it/s]\u001b[A\n",
      " 40%|████      | 167193/414113 [00:38<00:56, 4406.09it/s]\u001b[A\n",
      " 40%|████      | 167638/414113 [00:38<00:55, 4419.04it/s]\u001b[A\n",
      " 41%|████      | 168093/414113 [00:38<00:55, 4455.76it/s]\u001b[A\n",
      " 41%|████      | 168539/414113 [00:39<00:55, 4441.44it/s]\u001b[A\n",
      " 41%|████      | 168984/414113 [00:39<00:55, 4420.96it/s]\u001b[A\n",
      " 41%|████      | 169427/414113 [00:39<00:55, 4387.41it/s]\u001b[A\n",
      " 41%|████      | 169866/414113 [00:39<00:55, 4369.58it/s]\u001b[A\n",
      " 41%|████      | 170311/414113 [00:39<00:55, 4392.71it/s]\u001b[A\n",
      " 41%|████      | 170751/414113 [00:39<00:55, 4361.33it/s]\u001b[A\n",
      " 41%|████▏     | 171190/414113 [00:39<00:55, 4369.55it/s]\u001b[A\n",
      " 41%|████▏     | 171647/414113 [00:39<00:54, 4425.82it/s]\u001b[A\n",
      " 42%|████▏     | 172099/414113 [00:39<00:54, 4453.22it/s]\u001b[A\n",
      " 42%|████▏     | 172545/414113 [00:40<00:54, 4451.19it/s]\u001b[A\n",
      " 42%|████▏     | 172991/414113 [00:40<00:54, 4430.83it/s]\u001b[A\n",
      " 42%|████▏     | 173435/414113 [00:40<00:54, 4425.99it/s]\u001b[A\n",
      " 42%|████▏     | 173878/414113 [00:40<00:54, 4388.24it/s]\u001b[A\n",
      " 42%|████▏     | 174319/414113 [00:40<00:54, 4394.72it/s]\u001b[A\n",
      " 42%|████▏     | 174764/414113 [00:40<00:54, 4409.98it/s]\u001b[A\n",
      " 42%|████▏     | 175206/414113 [00:40<00:54, 4393.84it/s]\u001b[A\n",
      " 42%|████▏     | 175646/414113 [00:40<00:54, 4387.31it/s]\u001b[A\n",
      " 43%|████▎     | 176093/414113 [00:40<00:53, 4411.51it/s]\u001b[A\n",
      " 43%|████▎     | 176535/414113 [00:40<00:58, 4075.58it/s]\u001b[A\n",
      " 43%|████▎     | 176980/414113 [00:41<00:56, 4180.49it/s]\u001b[A\n",
      " 43%|████▎     | 177428/414113 [00:41<00:55, 4263.81it/s]\u001b[A\n",
      " 43%|████▎     | 177873/414113 [00:41<00:54, 4316.73it/s]\u001b[A\n",
      " 43%|████▎     | 178316/414113 [00:41<00:54, 4347.75it/s]\u001b[A\n",
      " 43%|████▎     | 178771/414113 [00:41<00:53, 4404.18it/s]\u001b[A\n",
      " 43%|████▎     | 179213/414113 [00:41<00:53, 4384.09it/s]\u001b[A\n",
      " 43%|████▎     | 179659/414113 [00:41<00:53, 4403.49it/s]\u001b[A\n",
      " 43%|████▎     | 180101/414113 [00:41<00:53, 4402.85it/s]\u001b[A\n",
      " 44%|████▎     | 180542/414113 [00:41<00:53, 4360.90it/s]\u001b[A\n",
      " 44%|████▎     | 180979/414113 [00:41<00:53, 4346.17it/s]\u001b[A\n",
      " 44%|████▍     | 181424/414113 [00:42<00:53, 4374.98it/s]\u001b[A\n",
      " 44%|████▍     | 181870/414113 [00:42<00:52, 4399.21it/s]\u001b[A\n",
      " 44%|████▍     | 182315/414113 [00:42<00:52, 4414.14it/s]\u001b[A\n",
      " 44%|████▍     | 182757/414113 [00:42<00:53, 4346.12it/s]\u001b[A\n",
      " 44%|████▍     | 183192/414113 [00:42<00:53, 4292.93it/s]\u001b[A\n",
      " 44%|████▍     | 183622/414113 [00:42<00:54, 4239.33it/s]\u001b[A\n",
      " 44%|████▍     | 184047/414113 [00:42<00:54, 4208.42it/s]\u001b[A\n",
      " 45%|████▍     | 184469/414113 [00:42<00:54, 4182.85it/s]\u001b[A\n",
      " 45%|████▍     | 184894/414113 [00:42<00:54, 4200.62it/s]\u001b[A\n",
      " 45%|████▍     | 185324/414113 [00:42<00:54, 4229.25it/s]\u001b[A\n",
      " 45%|████▍     | 185748/414113 [00:43<00:54, 4218.79it/s]\u001b[A\n",
      " 45%|████▍     | 186176/414113 [00:43<00:53, 4236.39it/s]\u001b[A\n",
      " 45%|████▌     | 186600/414113 [00:43<00:54, 4194.39it/s]\u001b[A\n",
      " 45%|████▌     | 187020/414113 [00:43<00:55, 4107.53it/s]\u001b[A\n",
      " 45%|████▌     | 187432/414113 [00:43<00:58, 3885.90it/s]\u001b[A\n",
      " 45%|████▌     | 187853/414113 [00:43<00:56, 3976.32it/s]\u001b[A\n",
      " 45%|████▌     | 188268/414113 [00:43<00:56, 4026.70it/s]\u001b[A\n",
      " 46%|████▌     | 188689/414113 [00:43<00:55, 4078.10it/s]\u001b[A\n",
      " 46%|████▌     | 189116/414113 [00:43<00:54, 4132.63it/s]\u001b[A\n",
      " 46%|████▌     | 189531/414113 [00:43<00:54, 4119.97it/s]\u001b[A\n",
      " 46%|████▌     | 189959/414113 [00:44<00:53, 4164.44it/s]\u001b[A\n",
      " 46%|████▌     | 190396/414113 [00:44<00:52, 4222.84it/s]\u001b[A\n",
      " 46%|████▌     | 190841/414113 [00:44<00:52, 4286.53it/s]\u001b[A\n",
      " 46%|████▌     | 191279/414113 [00:44<00:51, 4312.91it/s]\u001b[A\n",
      " 46%|████▋     | 191716/414113 [00:44<00:51, 4328.51it/s]\u001b[A\n",
      " 46%|████▋     | 192166/414113 [00:44<00:50, 4378.32it/s]\u001b[A\n",
      " 47%|████▋     | 192621/414113 [00:44<00:50, 4427.93it/s]\u001b[A\n",
      " 47%|████▋     | 193075/414113 [00:44<00:49, 4460.67it/s]\u001b[A\n",
      " 47%|████▋     | 193522/414113 [00:44<00:49, 4424.69it/s]\u001b[A\n",
      " 47%|████▋     | 193970/414113 [00:45<00:49, 4438.88it/s]\u001b[A\n",
      " 47%|████▋     | 194417/414113 [00:45<00:49, 4443.37it/s]\u001b[A\n",
      " 47%|████▋     | 194862/414113 [00:45<00:50, 4358.79it/s]\u001b[A\n",
      " 47%|████▋     | 195299/414113 [00:45<00:50, 4349.35it/s]\u001b[A\n",
      " 47%|████▋     | 195735/414113 [00:45<00:50, 4284.44it/s]\u001b[A\n",
      " 47%|████▋     | 196168/414113 [00:45<00:50, 4296.20it/s]\u001b[A\n",
      " 47%|████▋     | 196598/414113 [00:45<00:50, 4290.47it/s]\u001b[A\n",
      " 48%|████▊     | 197028/414113 [00:45<00:50, 4289.62it/s]\u001b[A\n",
      " 48%|████▊     | 197458/414113 [00:45<00:51, 4177.80it/s]\u001b[A\n",
      " 48%|████▊     | 197887/414113 [00:45<00:51, 4209.70it/s]\u001b[A\n",
      " 48%|████▊     | 198318/414113 [00:46<00:50, 4238.69it/s]\u001b[A\n",
      " 48%|████▊     | 198748/414113 [00:46<00:50, 4255.08it/s]\u001b[A\n",
      " 48%|████▊     | 199200/414113 [00:46<00:49, 4329.55it/s]\u001b[A\n",
      " 48%|████▊     | 199635/414113 [00:46<00:49, 4335.56it/s]\u001b[A\n",
      " 48%|████▊     | 200077/414113 [00:46<00:49, 4358.70it/s]\u001b[A\n",
      " 48%|████▊     | 200514/414113 [00:46<00:49, 4337.60it/s]\u001b[A\n",
      " 49%|████▊     | 200948/414113 [00:46<00:49, 4324.76it/s]\u001b[A\n",
      " 49%|████▊     | 201411/414113 [00:46<00:48, 4410.29it/s]\u001b[A\n",
      " 49%|████▊     | 201872/414113 [00:46<00:47, 4467.29it/s]\u001b[A\n",
      " 49%|████▉     | 202323/414113 [00:46<00:47, 4479.60it/s]\u001b[A\n",
      " 49%|████▉     | 202772/414113 [00:47<00:48, 4391.62it/s]\u001b[A\n",
      " 49%|████▉     | 203212/414113 [00:47<00:49, 4245.08it/s]\u001b[A\n",
      " 49%|████▉     | 203643/414113 [00:47<00:49, 4262.35it/s]\u001b[A\n",
      " 49%|████▉     | 204088/414113 [00:47<00:48, 4315.40it/s]\u001b[A\n",
      " 49%|████▉     | 204540/414113 [00:47<00:47, 4373.98it/s]\u001b[A\n",
      " 50%|████▉     | 204995/414113 [00:47<00:47, 4422.21it/s]\u001b[A\n",
      " 50%|████▉     | 205443/414113 [00:47<00:47, 4438.12it/s]\u001b[A\n",
      " 50%|████▉     | 205888/414113 [00:47<00:46, 4439.21it/s]\u001b[A\n",
      " 50%|████▉     | 206333/414113 [00:47<00:46, 4440.79it/s]\u001b[A\n",
      " 50%|████▉     | 206778/414113 [00:47<00:46, 4429.18it/s]\u001b[A\n",
      " 50%|█████     | 207222/414113 [00:48<00:46, 4405.13it/s]\u001b[A\n",
      " 50%|█████     | 207663/414113 [00:48<00:46, 4394.43it/s]\u001b[A\n",
      " 50%|█████     | 208114/414113 [00:48<00:46, 4426.50it/s]\u001b[A\n",
      " 50%|█████     | 208557/414113 [00:48<00:46, 4376.83it/s]\u001b[A\n",
      " 50%|█████     | 208995/414113 [00:48<00:47, 4364.12it/s]\u001b[A\n",
      " 51%|█████     | 209445/414113 [00:48<00:46, 4396.60it/s]\u001b[A\n",
      " 51%|█████     | 209894/414113 [00:48<00:46, 4422.26it/s]\u001b[A\n",
      " 51%|█████     | 210355/414113 [00:48<00:45, 4476.42it/s]\u001b[A\n",
      " 51%|█████     | 210814/414113 [00:48<00:45, 4508.41it/s]\u001b[A\n",
      " 51%|█████     | 211266/414113 [00:48<00:45, 4496.66it/s]\u001b[A\n",
      " 51%|█████     | 211716/414113 [00:49<00:45, 4494.41it/s]\u001b[A\n",
      " 51%|█████     | 212166/414113 [00:49<00:45, 4481.87it/s]\u001b[A\n",
      " 51%|█████▏    | 212615/414113 [00:49<00:45, 4477.08it/s]\u001b[A\n",
      " 51%|█████▏    | 213063/414113 [00:49<00:45, 4462.37it/s]\u001b[A\n",
      " 52%|█████▏    | 213510/414113 [00:49<00:45, 4427.71it/s]\u001b[A\n",
      " 52%|█████▏    | 213953/414113 [00:49<00:45, 4407.78it/s]\u001b[A\n",
      " 52%|█████▏    | 214394/414113 [00:49<00:45, 4386.73it/s]\u001b[A\n",
      " 52%|█████▏    | 214833/414113 [00:49<00:46, 4273.88it/s]\u001b[A\n",
      " 52%|█████▏    | 215275/414113 [00:49<00:46, 4316.22it/s]\u001b[A\n",
      " 52%|█████▏    | 215718/414113 [00:49<00:45, 4349.67it/s]\u001b[A\n",
      " 52%|█████▏    | 216169/414113 [00:50<00:45, 4393.68it/s]\u001b[A\n",
      " 52%|█████▏    | 216614/414113 [00:50<00:44, 4409.37it/s]\u001b[A\n",
      " 52%|█████▏    | 217063/414113 [00:50<00:44, 4430.17it/s]\u001b[A\n",
      " 53%|█████▎    | 217507/414113 [00:50<00:44, 4390.26it/s]\u001b[A\n",
      " 53%|█████▎    | 217947/414113 [00:50<00:44, 4385.20it/s]\u001b[A\n",
      " 53%|█████▎    | 218411/414113 [00:50<00:43, 4458.46it/s]\u001b[A\n",
      " 53%|█████▎    | 218867/414113 [00:50<00:43, 4488.37it/s]\u001b[A\n",
      " 53%|█████▎    | 219337/414113 [00:50<00:42, 4549.60it/s]\u001b[A\n",
      " 53%|█████▎    | 219793/414113 [00:50<00:42, 4533.06it/s]\u001b[A\n",
      " 53%|█████▎    | 220247/414113 [00:50<00:43, 4465.43it/s]\u001b[A\n",
      " 53%|█████▎    | 220700/414113 [00:51<00:43, 4481.76it/s]\u001b[A\n",
      " 53%|█████▎    | 221149/414113 [00:51<00:43, 4445.48it/s]\u001b[A\n",
      " 54%|█████▎    | 221594/414113 [00:51<00:43, 4441.13it/s]\u001b[A\n",
      " 54%|█████▎    | 222049/414113 [00:51<00:42, 4472.76it/s]\u001b[A\n",
      " 54%|█████▎    | 222504/414113 [00:51<00:42, 4494.83it/s]\u001b[A\n",
      " 54%|█████▍    | 222973/414113 [00:51<00:42, 4550.94it/s]\u001b[A\n",
      " 54%|█████▍    | 223430/414113 [00:51<00:41, 4556.30it/s]\u001b[A\n",
      " 54%|█████▍    | 223886/414113 [00:51<00:41, 4548.16it/s]\u001b[A\n",
      " 54%|█████▍    | 224352/414113 [00:51<00:41, 4580.34it/s]\u001b[A\n",
      " 54%|█████▍    | 224811/414113 [00:51<00:41, 4562.04it/s]\u001b[A\n",
      " 54%|█████▍    | 225274/414113 [00:52<00:41, 4580.59it/s]\u001b[A\n",
      " 55%|█████▍    | 225733/414113 [00:52<00:41, 4548.89it/s]\u001b[A\n",
      " 55%|█████▍    | 226189/414113 [00:52<00:41, 4531.38it/s]\u001b[A\n",
      " 55%|█████▍    | 226643/414113 [00:52<00:41, 4532.77it/s]\u001b[A\n",
      " 55%|█████▍    | 227102/414113 [00:52<00:41, 4547.70it/s]\u001b[A\n",
      " 55%|█████▍    | 227569/414113 [00:52<00:40, 4581.19it/s]\u001b[A\n",
      " 55%|█████▌    | 228028/414113 [00:52<00:40, 4580.35it/s]\u001b[A\n",
      " 55%|█████▌    | 228487/414113 [00:52<00:40, 4562.39it/s]\u001b[A\n",
      " 55%|█████▌    | 228952/414113 [00:52<00:40, 4587.79it/s]\u001b[A\n",
      " 55%|█████▌    | 229411/414113 [00:53<00:40, 4548.95it/s]\u001b[A\n",
      " 56%|█████▌    | 229867/414113 [00:53<00:40, 4521.84it/s]\u001b[A\n",
      " 56%|█████▌    | 230320/414113 [00:53<00:40, 4522.21it/s]\u001b[A\n",
      " 56%|█████▌    | 230773/414113 [00:53<00:43, 4247.12it/s]\u001b[A\n",
      " 56%|█████▌    | 231202/414113 [00:53<00:42, 4258.34it/s]\u001b[A\n",
      " 56%|█████▌    | 231640/414113 [00:53<00:42, 4292.77it/s]\u001b[A\n",
      " 56%|█████▌    | 232082/414113 [00:53<00:42, 4329.26it/s]\u001b[A\n",
      " 56%|█████▌    | 232517/414113 [00:53<00:43, 4174.31it/s]\u001b[A\n",
      " 56%|█████▌    | 232937/414113 [00:53<00:43, 4180.56it/s]\u001b[A\n",
      " 56%|█████▋    | 233366/414113 [00:53<00:42, 4210.30it/s]\u001b[A\n",
      " 56%|█████▋    | 233799/414113 [00:54<00:42, 4243.61it/s]\u001b[A\n",
      " 57%|█████▋    | 234237/414113 [00:54<00:42, 4281.25it/s]\u001b[A\n",
      " 57%|█████▋    | 234688/414113 [00:54<00:41, 4345.55it/s]\u001b[A\n",
      " 57%|█████▋    | 235124/414113 [00:54<00:41, 4306.64it/s]\u001b[A\n",
      " 57%|█████▋    | 235556/414113 [00:54<00:41, 4269.42it/s]\u001b[A\n",
      " 57%|█████▋    | 235984/414113 [00:54<00:41, 4258.37it/s]\u001b[A\n",
      " 57%|█████▋    | 236411/414113 [00:54<00:43, 4098.18it/s]\u001b[A\n",
      " 57%|█████▋    | 236847/414113 [00:54<00:42, 4171.33it/s]\u001b[A\n",
      " 57%|█████▋    | 237294/414113 [00:54<00:41, 4254.96it/s]\u001b[A\n",
      " 57%|█████▋    | 237729/414113 [00:54<00:41, 4280.66it/s]\u001b[A\n",
      " 58%|█████▊    | 238159/414113 [00:55<00:41, 4286.02it/s]\u001b[A\n",
      " 58%|█████▊    | 238589/414113 [00:55<00:41, 4213.50it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 239023/414113 [00:55<00:41, 4249.02it/s]\u001b[A\n",
      " 58%|█████▊    | 239464/414113 [00:55<00:40, 4295.61it/s]\u001b[A\n",
      " 58%|█████▊    | 239896/414113 [00:55<00:40, 4300.94it/s]\u001b[A\n",
      " 58%|█████▊    | 240339/414113 [00:55<00:40, 4336.17it/s]\u001b[A\n",
      " 58%|█████▊    | 240788/414113 [00:55<00:39, 4379.85it/s]\u001b[A\n",
      " 58%|█████▊    | 241237/414113 [00:55<00:39, 4410.33it/s]\u001b[A\n",
      " 58%|█████▊    | 241679/414113 [00:55<00:39, 4359.22it/s]\u001b[A\n",
      " 58%|█████▊    | 242130/414113 [00:55<00:39, 4401.47it/s]\u001b[A\n",
      " 59%|█████▊    | 242572/414113 [00:56<00:38, 4406.76it/s]\u001b[A\n",
      " 59%|█████▊    | 243013/414113 [00:56<00:39, 4351.97it/s]\u001b[A\n",
      " 59%|█████▉    | 243466/414113 [00:56<00:38, 4401.10it/s]\u001b[A\n",
      " 59%|█████▉    | 243907/414113 [00:56<00:38, 4401.64it/s]\u001b[A\n",
      " 59%|█████▉    | 244354/414113 [00:56<00:38, 4420.04it/s]\u001b[A\n",
      " 59%|█████▉    | 244797/414113 [00:56<00:38, 4381.46it/s]\u001b[A\n",
      " 59%|█████▉    | 245239/414113 [00:56<00:38, 4390.83it/s]\u001b[A\n",
      " 59%|█████▉    | 245695/414113 [00:56<00:37, 4438.05it/s]\u001b[A\n",
      " 59%|█████▉    | 246140/414113 [00:56<00:37, 4440.24it/s]\u001b[A\n",
      " 60%|█████▉    | 246585/414113 [00:56<00:37, 4438.40it/s]\u001b[A\n",
      " 60%|█████▉    | 247029/414113 [00:57<00:38, 4331.00it/s]\u001b[A\n",
      " 60%|█████▉    | 247463/414113 [00:57<00:38, 4289.70it/s]\u001b[A\n",
      " 60%|█████▉    | 247893/414113 [00:57<00:39, 4206.61it/s]\u001b[A\n",
      " 60%|█████▉    | 248323/414113 [00:57<00:39, 4234.14it/s]\u001b[A\n",
      " 60%|██████    | 248749/414113 [00:57<00:38, 4240.49it/s]\u001b[A\n",
      " 60%|██████    | 249174/414113 [00:57<00:39, 4217.62it/s]\u001b[A\n",
      " 60%|██████    | 249597/414113 [00:57<00:39, 4180.46it/s]\u001b[A\n",
      " 60%|██████    | 250016/414113 [00:57<00:41, 3952.57it/s]\u001b[A\n",
      " 60%|██████    | 250451/414113 [00:57<00:40, 4061.32it/s]\u001b[A\n",
      " 61%|██████    | 250913/414113 [00:58<00:38, 4212.55it/s]\u001b[A\n",
      " 61%|██████    | 251348/414113 [00:58<00:38, 4252.25it/s]\u001b[A\n",
      " 61%|██████    | 251797/414113 [00:58<00:37, 4318.67it/s]\u001b[A\n",
      " 61%|██████    | 252233/414113 [00:58<00:37, 4329.50it/s]\u001b[A\n",
      " 61%|██████    | 252668/414113 [00:58<00:37, 4334.66it/s]\u001b[A\n",
      " 61%|██████    | 253103/414113 [00:58<00:37, 4312.14it/s]\u001b[A\n",
      " 61%|██████    | 253535/414113 [00:58<00:37, 4309.34it/s]\u001b[A\n",
      " 61%|██████▏   | 253967/414113 [00:58<00:37, 4294.73it/s]\u001b[A\n",
      " 61%|██████▏   | 254412/414113 [00:58<00:36, 4339.98it/s]\u001b[A\n",
      " 62%|██████▏   | 254866/414113 [00:58<00:36, 4397.99it/s]\u001b[A\n",
      " 62%|██████▏   | 255314/414113 [00:59<00:35, 4422.02it/s]\u001b[A\n",
      " 62%|██████▏   | 255769/414113 [00:59<00:35, 4458.09it/s]\u001b[A\n",
      " 62%|██████▏   | 256223/414113 [00:59<00:35, 4481.71it/s]\u001b[A\n",
      " 62%|██████▏   | 256680/414113 [00:59<00:34, 4505.45it/s]\u001b[A\n",
      " 62%|██████▏   | 257136/414113 [00:59<00:34, 4519.70it/s]\u001b[A\n",
      " 62%|██████▏   | 257589/414113 [00:59<00:34, 4482.76it/s]\u001b[A\n",
      " 62%|██████▏   | 258042/414113 [00:59<00:34, 4496.67it/s]\u001b[A\n",
      " 62%|██████▏   | 258492/414113 [00:59<00:34, 4462.25it/s]\u001b[A\n",
      " 63%|██████▎   | 258939/414113 [00:59<00:34, 4434.88it/s]\u001b[A\n",
      " 63%|██████▎   | 259383/414113 [00:59<00:35, 4415.16it/s]\u001b[A\n",
      " 63%|██████▎   | 259832/414113 [01:00<00:34, 4436.29it/s]\u001b[A\n",
      " 63%|██████▎   | 260276/414113 [01:00<00:34, 4419.30it/s]\u001b[A\n",
      " 63%|██████▎   | 260741/414113 [01:00<00:34, 4485.91it/s]\u001b[A\n",
      " 63%|██████▎   | 261197/414113 [01:00<00:33, 4506.55it/s]\u001b[A\n",
      " 63%|██████▎   | 261648/414113 [01:00<00:33, 4505.06it/s]\u001b[A\n",
      " 63%|██████▎   | 262099/414113 [01:00<00:59, 2535.03it/s]\u001b[A\n",
      " 63%|██████▎   | 262550/414113 [01:00<00:51, 2916.85it/s]\u001b[A\n",
      " 64%|██████▎   | 262993/414113 [01:01<00:46, 3248.20it/s]\u001b[A\n",
      " 64%|██████▎   | 263428/414113 [01:01<00:42, 3514.39it/s]\u001b[A\n",
      " 64%|██████▎   | 263875/414113 [01:01<00:40, 3754.82it/s]\u001b[A\n",
      " 64%|██████▍   | 264307/414113 [01:01<00:38, 3906.77it/s]\u001b[A\n",
      " 64%|██████▍   | 264750/414113 [01:01<00:36, 4048.46it/s]\u001b[A\n",
      " 64%|██████▍   | 265190/414113 [01:01<00:35, 4147.00it/s]\u001b[A\n",
      " 64%|██████▍   | 265633/414113 [01:01<00:35, 4227.60it/s]\u001b[A\n",
      " 64%|██████▍   | 266077/414113 [01:01<00:34, 4288.22it/s]\u001b[A\n",
      " 64%|██████▍   | 266525/414113 [01:01<00:33, 4342.79it/s]\u001b[A\n",
      " 64%|██████▍   | 266966/414113 [01:01<00:34, 4288.35it/s]\u001b[A\n",
      " 65%|██████▍   | 267400/414113 [01:02<00:35, 4086.44it/s]\u001b[A\n",
      " 65%|██████▍   | 267816/414113 [01:02<00:35, 4104.80it/s]\u001b[A\n",
      " 65%|██████▍   | 268246/414113 [01:02<00:35, 4160.86it/s]\u001b[A\n",
      " 65%|██████▍   | 268692/414113 [01:02<00:34, 4245.20it/s]\u001b[A\n",
      " 65%|██████▍   | 269119/414113 [01:02<00:34, 4232.45it/s]\u001b[A\n",
      " 65%|██████▌   | 269544/414113 [01:02<00:34, 4218.76it/s]\u001b[A\n",
      " 65%|██████▌   | 269982/414113 [01:02<00:33, 4263.80it/s]\u001b[A\n",
      " 65%|██████▌   | 270432/414113 [01:02<00:33, 4329.78it/s]\u001b[A\n",
      " 65%|██████▌   | 270894/414113 [01:02<00:32, 4412.03it/s]\u001b[A\n",
      " 66%|██████▌   | 271337/414113 [01:02<00:32, 4381.02it/s]\u001b[A\n",
      " 66%|██████▌   | 271776/414113 [01:03<00:32, 4379.58it/s]\u001b[A\n",
      " 66%|██████▌   | 272215/414113 [01:03<00:32, 4364.85it/s]\u001b[A\n",
      " 66%|██████▌   | 272670/414113 [01:03<00:32, 4417.30it/s]\u001b[A\n",
      " 66%|██████▌   | 273124/414113 [01:03<00:31, 4449.92it/s]\u001b[A\n",
      " 66%|██████▌   | 273581/414113 [01:03<00:31, 4484.46it/s]\u001b[A\n",
      " 66%|██████▌   | 274030/414113 [01:03<00:31, 4480.33it/s]\u001b[A\n",
      " 66%|██████▋   | 274486/414113 [01:03<00:31, 4503.03it/s]\u001b[A\n",
      " 66%|██████▋   | 274937/414113 [01:03<00:30, 4501.47it/s]\u001b[A\n",
      " 67%|██████▋   | 275388/414113 [01:03<00:30, 4495.99it/s]\u001b[A\n",
      " 67%|██████▋   | 275838/414113 [01:03<00:31, 4449.67it/s]\u001b[A\n",
      " 67%|██████▋   | 276284/414113 [01:04<00:30, 4448.25it/s]\u001b[A\n",
      " 67%|██████▋   | 276729/414113 [01:04<00:31, 4353.51it/s]\u001b[A\n",
      " 67%|██████▋   | 277180/414113 [01:04<00:31, 4398.90it/s]\u001b[A\n",
      " 67%|██████▋   | 277635/414113 [01:04<00:30, 4442.74it/s]\u001b[A\n",
      " 67%|██████▋   | 278080/414113 [01:04<00:30, 4438.65it/s]\u001b[A\n",
      " 67%|██████▋   | 278525/414113 [01:04<00:30, 4398.62it/s]\u001b[A\n",
      " 67%|██████▋   | 278970/414113 [01:04<00:30, 4412.69it/s]\u001b[A\n",
      " 67%|██████▋   | 279412/414113 [01:04<00:30, 4395.66it/s]\u001b[A\n",
      " 68%|██████▊   | 279852/414113 [01:04<00:32, 4168.90it/s]\u001b[A\n",
      " 68%|██████▊   | 280293/414113 [01:04<00:31, 4237.78it/s]\u001b[A\n",
      " 68%|██████▊   | 280723/414113 [01:05<00:31, 4255.99it/s]\u001b[A\n",
      " 68%|██████▊   | 281153/414113 [01:05<00:31, 4268.76it/s]\u001b[A\n",
      " 68%|██████▊   | 281606/414113 [01:05<00:30, 4342.58it/s]\u001b[A\n",
      " 68%|██████▊   | 282048/414113 [01:05<00:30, 4363.89it/s]\u001b[A\n",
      " 68%|██████▊   | 282486/414113 [01:05<00:30, 4350.09it/s]\u001b[A\n",
      " 68%|██████▊   | 282922/414113 [01:05<00:30, 4317.82it/s]\u001b[A\n",
      " 68%|██████▊   | 283376/414113 [01:05<00:29, 4381.38it/s]\u001b[A\n",
      " 69%|██████▊   | 283823/414113 [01:05<00:29, 4405.53it/s]\u001b[A\n",
      " 69%|██████▊   | 284273/414113 [01:05<00:29, 4432.05it/s]\u001b[A\n",
      " 69%|██████▉   | 284720/414113 [01:05<00:29, 4443.13it/s]\u001b[A\n",
      " 69%|██████▉   | 285165/414113 [01:06<00:29, 4397.52it/s]\u001b[A\n",
      " 69%|██████▉   | 285606/414113 [01:06<00:29, 4322.23it/s]\u001b[A\n",
      " 69%|██████▉   | 286039/414113 [01:06<00:29, 4304.58it/s]\u001b[A\n",
      " 69%|██████▉   | 286473/414113 [01:06<00:29, 4313.24it/s]\u001b[A\n",
      " 69%|██████▉   | 286930/414113 [01:06<00:29, 4385.36it/s]\u001b[A\n",
      " 69%|██████▉   | 287371/414113 [01:06<00:28, 4390.40it/s]\u001b[A\n",
      " 70%|██████▉   | 287815/414113 [01:06<00:28, 4402.86it/s]\u001b[A\n",
      " 70%|██████▉   | 288256/414113 [01:06<00:29, 4310.84it/s]\u001b[A\n",
      " 70%|██████▉   | 288688/414113 [01:06<00:29, 4285.78it/s]\u001b[A\n",
      " 70%|██████▉   | 289125/414113 [01:06<00:29, 4309.24it/s]\u001b[A\n",
      " 70%|██████▉   | 289557/414113 [01:07<00:28, 4298.80it/s]\u001b[A\n",
      " 70%|███████   | 289988/414113 [01:07<00:29, 4258.94it/s]\u001b[A\n",
      " 70%|███████   | 290415/414113 [01:07<00:29, 4255.83it/s]\u001b[A\n",
      " 70%|███████   | 290841/414113 [01:07<00:28, 4250.99it/s]\u001b[A\n",
      " 70%|███████   | 291274/414113 [01:07<00:28, 4273.05it/s]\u001b[A\n",
      " 70%|███████   | 291702/414113 [01:07<00:28, 4262.53it/s]\u001b[A\n",
      " 71%|███████   | 292129/414113 [01:07<00:28, 4259.18it/s]\u001b[A\n",
      " 71%|███████   | 292555/414113 [01:07<00:28, 4256.54it/s]\u001b[A\n",
      " 71%|███████   | 293002/414113 [01:07<00:28, 4317.51it/s]\u001b[A\n",
      " 71%|███████   | 293470/414113 [01:08<00:27, 4419.66it/s]\u001b[A\n",
      " 71%|███████   | 293913/414113 [01:08<00:27, 4366.38it/s]\u001b[A\n",
      " 71%|███████   | 294351/414113 [01:08<00:27, 4333.32it/s]\u001b[A\n",
      " 71%|███████   | 294785/414113 [01:08<00:27, 4313.10it/s]\u001b[A\n",
      " 71%|███████▏  | 295238/414113 [01:08<00:27, 4374.36it/s]\u001b[A\n",
      " 71%|███████▏  | 295683/414113 [01:08<00:26, 4394.59it/s]\u001b[A\n",
      " 72%|███████▏  | 296137/414113 [01:08<00:26, 4434.73it/s]\u001b[A\n",
      " 72%|███████▏  | 296581/414113 [01:08<00:27, 4308.69it/s]\u001b[A\n",
      " 72%|███████▏  | 297024/414113 [01:08<00:26, 4343.37it/s]\u001b[A\n",
      " 72%|███████▏  | 297460/414113 [01:08<00:26, 4326.65it/s]\u001b[A\n",
      " 72%|███████▏  | 297894/414113 [01:09<00:26, 4319.89it/s]\u001b[A\n",
      " 72%|███████▏  | 298337/414113 [01:09<00:26, 4351.74it/s]\u001b[A\n",
      " 72%|███████▏  | 298778/414113 [01:09<00:26, 4367.83it/s]\u001b[A\n",
      " 72%|███████▏  | 299218/414113 [01:09<00:26, 4375.99it/s]\u001b[A\n",
      " 72%|███████▏  | 299670/414113 [01:09<00:25, 4417.99it/s]\u001b[A\n",
      " 72%|███████▏  | 300113/414113 [01:09<00:25, 4385.72it/s]\u001b[A\n",
      " 73%|███████▎  | 300552/414113 [01:09<00:26, 4310.49it/s]\u001b[A\n",
      " 73%|███████▎  | 300984/414113 [01:09<00:26, 4305.96it/s]\u001b[A\n",
      " 73%|███████▎  | 301415/414113 [01:09<00:26, 4268.40it/s]\u001b[A\n",
      " 73%|███████▎  | 301844/414113 [01:09<00:26, 4272.15it/s]\u001b[A\n",
      " 73%|███████▎  | 302293/414113 [01:10<00:25, 4334.63it/s]\u001b[A\n",
      " 73%|███████▎  | 302727/414113 [01:10<00:25, 4314.53it/s]\u001b[A\n",
      " 73%|███████▎  | 303177/414113 [01:10<00:25, 4367.61it/s]\u001b[A\n",
      " 73%|███████▎  | 303616/414113 [01:10<00:25, 4371.74it/s]\u001b[A\n",
      " 73%|███████▎  | 304055/414113 [01:10<00:25, 4375.82it/s]\u001b[A\n",
      " 74%|███████▎  | 304502/414113 [01:10<00:24, 4401.83it/s]\u001b[A\n",
      " 74%|███████▎  | 304943/414113 [01:10<00:24, 4400.10it/s]\u001b[A\n",
      " 74%|███████▎  | 305391/414113 [01:10<00:24, 4421.69it/s]\u001b[A\n",
      " 74%|███████▍  | 305834/414113 [01:10<00:24, 4400.45it/s]\u001b[A\n",
      " 74%|███████▍  | 306283/414113 [01:10<00:24, 4424.39it/s]\u001b[A\n",
      " 74%|███████▍  | 306726/414113 [01:11<00:24, 4404.84it/s]\u001b[A\n",
      " 74%|███████▍  | 307167/414113 [01:11<00:24, 4350.58it/s]\u001b[A\n",
      " 74%|███████▍  | 307603/414113 [01:11<00:24, 4303.21it/s]\u001b[A\n",
      " 74%|███████▍  | 308034/414113 [01:11<00:25, 4233.11it/s]\u001b[A\n",
      " 74%|███████▍  | 308459/414113 [01:11<00:24, 4235.94it/s]\u001b[A\n",
      " 75%|███████▍  | 308883/414113 [01:11<00:25, 4170.25it/s]\u001b[A\n",
      " 75%|███████▍  | 309301/414113 [01:11<00:25, 4041.04it/s]\u001b[A\n",
      " 75%|███████▍  | 309707/414113 [01:11<00:25, 4032.63it/s]\u001b[A\n",
      " 75%|███████▍  | 310144/414113 [01:11<00:25, 4125.48it/s]\u001b[A\n",
      " 75%|███████▍  | 310564/414113 [01:11<00:24, 4147.01it/s]\u001b[A\n",
      " 75%|███████▌  | 310980/414113 [01:12<00:24, 4147.66it/s]\u001b[A\n",
      " 75%|███████▌  | 311424/414113 [01:12<00:24, 4229.70it/s]\u001b[A\n",
      " 75%|███████▌  | 311875/414113 [01:12<00:23, 4309.39it/s]\u001b[A\n",
      " 75%|███████▌  | 312307/414113 [01:12<00:23, 4291.71it/s]\u001b[A\n",
      " 76%|███████▌  | 312737/414113 [01:12<00:23, 4279.18it/s]\u001b[A\n",
      " 76%|███████▌  | 313166/414113 [01:12<00:23, 4276.55it/s]\u001b[A\n",
      " 76%|███████▌  | 313594/414113 [01:12<00:23, 4271.90it/s]\u001b[A\n",
      " 76%|███████▌  | 314022/414113 [01:12<00:23, 4239.07it/s]\u001b[A\n",
      " 76%|███████▌  | 314455/414113 [01:12<00:23, 4264.68it/s]\u001b[A\n",
      " 76%|███████▌  | 314892/414113 [01:12<00:23, 4294.31it/s]\u001b[A\n",
      " 76%|███████▌  | 315334/414113 [01:13<00:22, 4329.55it/s]\u001b[A\n",
      " 76%|███████▋  | 315790/414113 [01:13<00:22, 4393.45it/s]\u001b[A\n",
      " 76%|███████▋  | 316230/414113 [01:13<00:22, 4356.11it/s]\u001b[A\n",
      " 76%|███████▋  | 316688/414113 [01:13<00:22, 4419.75it/s]\u001b[A\n",
      " 77%|███████▋  | 317131/414113 [01:13<00:21, 4422.75it/s]\u001b[A\n",
      " 77%|███████▋  | 317589/414113 [01:13<00:21, 4467.93it/s]\u001b[A\n",
      " 77%|███████▋  | 318037/414113 [01:13<00:21, 4463.60it/s]\u001b[A\n",
      " 77%|███████▋  | 318484/414113 [01:13<00:21, 4451.12it/s]\u001b[A\n",
      " 77%|███████▋  | 318931/414113 [01:13<00:21, 4453.08it/s]\u001b[A\n",
      " 77%|███████▋  | 319386/414113 [01:13<00:21, 4480.35it/s]\u001b[A\n",
      " 77%|███████▋  | 319841/414113 [01:14<00:20, 4498.28it/s]\u001b[A\n",
      " 77%|███████▋  | 320291/414113 [01:14<00:20, 4493.06it/s]\u001b[A\n",
      " 77%|███████▋  | 320741/414113 [01:14<00:21, 4420.87it/s]\u001b[A\n",
      " 78%|███████▊  | 321184/414113 [01:14<00:21, 4376.34it/s]\u001b[A\n",
      " 78%|███████▊  | 321622/414113 [01:14<00:21, 4349.38it/s]\u001b[A\n",
      " 78%|███████▊  | 322067/414113 [01:14<00:21, 4378.16it/s]\u001b[A\n",
      " 78%|███████▊  | 322519/414113 [01:14<00:20, 4419.59it/s]\u001b[A\n",
      " 78%|███████▊  | 322964/414113 [01:14<00:20, 4428.56it/s]\u001b[A\n",
      " 78%|███████▊  | 323416/414113 [01:14<00:20, 4455.32it/s]\u001b[A\n",
      " 78%|███████▊  | 323862/414113 [01:15<00:20, 4405.27it/s]\u001b[A\n",
      " 78%|███████▊  | 324303/414113 [01:15<00:20, 4399.42it/s]\u001b[A\n",
      " 78%|███████▊  | 324744/414113 [01:15<00:20, 4339.06it/s]\u001b[A\n",
      " 79%|███████▊  | 325194/414113 [01:15<00:20, 4384.20it/s]\u001b[A\n",
      " 79%|███████▊  | 325633/414113 [01:15<00:20, 4356.21it/s]\u001b[A\n",
      " 79%|███████▊  | 326085/414113 [01:15<00:19, 4402.89it/s]\u001b[A\n",
      " 79%|███████▉  | 326526/414113 [01:15<00:19, 4404.64it/s]\u001b[A\n",
      " 79%|███████▉  | 326976/414113 [01:15<00:19, 4432.33it/s]\u001b[A\n",
      " 79%|███████▉  | 327427/414113 [01:15<00:19, 4452.21it/s]\u001b[A\n",
      " 79%|███████▉  | 327873/414113 [01:15<00:19, 4432.58it/s]\u001b[A\n",
      " 79%|███████▉  | 328317/414113 [01:16<00:19, 4356.96it/s]\u001b[A\n",
      " 79%|███████▉  | 328754/414113 [01:16<00:19, 4335.97it/s]\u001b[A\n",
      " 79%|███████▉  | 329189/414113 [01:16<00:19, 4337.50it/s]\u001b[A\n",
      " 80%|███████▉  | 329633/414113 [01:16<00:19, 4367.09it/s]\u001b[A\n",
      " 80%|███████▉  | 330070/414113 [01:16<00:19, 4367.14it/s]\u001b[A\n",
      " 80%|███████▉  | 330507/414113 [01:16<00:19, 4366.93it/s]\u001b[A\n",
      " 80%|███████▉  | 330944/414113 [01:16<00:19, 4341.67it/s]\u001b[A\n",
      " 80%|████████  | 331379/414113 [01:16<00:19, 4270.06it/s]\u001b[A\n",
      " 80%|████████  | 331807/414113 [01:16<00:19, 4237.03it/s]\u001b[A\n",
      " 80%|████████  | 332231/414113 [01:16<00:19, 4173.55it/s]\u001b[A\n",
      " 80%|████████  | 332649/414113 [01:17<00:19, 4106.96it/s]\u001b[A\n",
      " 80%|████████  | 333061/414113 [01:17<00:20, 4048.88it/s]\u001b[A\n",
      " 81%|████████  | 333480/414113 [01:17<00:19, 4088.93it/s]\u001b[A\n",
      " 81%|████████  | 333890/414113 [01:17<00:19, 4040.83it/s]\u001b[A\n",
      " 81%|████████  | 334306/414113 [01:17<00:19, 4073.70it/s]\u001b[A\n",
      " 81%|████████  | 334714/414113 [01:17<00:19, 4046.71it/s]\u001b[A\n",
      " 81%|████████  | 335134/414113 [01:17<00:19, 4089.87it/s]\u001b[A\n",
      " 81%|████████  | 335544/414113 [01:17<00:19, 4046.58it/s]\u001b[A\n",
      " 81%|████████  | 335967/414113 [01:17<00:19, 4099.58it/s]\u001b[A\n",
      " 81%|████████  | 336395/414113 [01:17<00:18, 4150.63it/s]\u001b[A\n",
      " 81%|████████▏ | 336828/414113 [01:18<00:18, 4202.42it/s]\u001b[A\n",
      " 81%|████████▏ | 337249/414113 [01:18<00:18, 4202.34it/s]\u001b[A\n",
      " 82%|████████▏ | 337684/414113 [01:18<00:18, 4243.73it/s]\u001b[A\n",
      " 82%|████████▏ | 338118/414113 [01:18<00:17, 4269.14it/s]\u001b[A\n",
      " 82%|████████▏ | 338550/414113 [01:18<00:17, 4281.53it/s]\u001b[A\n",
      " 82%|████████▏ | 338997/414113 [01:18<00:17, 4336.32it/s]\u001b[A\n",
      " 82%|████████▏ | 339431/414113 [01:18<00:17, 4308.44it/s]\u001b[A\n",
      " 82%|████████▏ | 339863/414113 [01:18<00:17, 4291.30it/s]\u001b[A\n",
      " 82%|████████▏ | 340297/414113 [01:18<00:17, 4303.77it/s]\u001b[A\n",
      " 82%|████████▏ | 340731/414113 [01:18<00:17, 4313.35it/s]\u001b[A\n",
      " 82%|████████▏ | 341175/414113 [01:19<00:16, 4350.12it/s]\u001b[A\n",
      " 82%|████████▏ | 341612/414113 [01:19<00:16, 4354.86it/s]\u001b[A\n",
      " 83%|████████▎ | 342052/414113 [01:19<00:16, 4365.82it/s]\u001b[A\n",
      " 83%|████████▎ | 342496/414113 [01:19<00:16, 4385.67it/s]\u001b[A\n",
      " 83%|████████▎ | 342943/414113 [01:19<00:16, 4408.44it/s]\u001b[A\n",
      " 83%|████████▎ | 343384/414113 [01:19<00:16, 4387.59it/s]\u001b[A\n",
      " 83%|████████▎ | 343837/414113 [01:19<00:15, 4427.27it/s]\u001b[A\n",
      " 83%|████████▎ | 344280/414113 [01:19<00:15, 4385.29it/s]\u001b[A\n",
      " 83%|████████▎ | 344719/414113 [01:19<00:15, 4368.36it/s]\u001b[A\n",
      " 83%|████████▎ | 345169/414113 [01:19<00:15, 4404.18it/s]\u001b[A\n",
      " 83%|████████▎ | 345610/414113 [01:20<00:16, 4071.85it/s]\u001b[A\n",
      " 84%|████████▎ | 346061/414113 [01:20<00:16, 4192.53it/s]\u001b[A\n",
      " 84%|████████▎ | 346504/414113 [01:20<00:15, 4258.34it/s]\u001b[A\n",
      " 84%|████████▍ | 346944/414113 [01:20<00:15, 4297.67it/s]\u001b[A\n",
      " 84%|████████▍ | 347383/414113 [01:20<00:15, 4321.72it/s]\u001b[A\n",
      " 84%|████████▍ | 347826/414113 [01:20<00:15, 4352.03it/s]\u001b[A\n",
      " 84%|████████▍ | 348265/414113 [01:20<00:15, 4363.29it/s]\u001b[A\n",
      " 84%|████████▍ | 348703/414113 [01:20<00:15, 4357.62it/s]\u001b[A\n",
      " 84%|████████▍ | 349141/414113 [01:20<00:14, 4362.47it/s]\u001b[A\n",
      " 84%|████████▍ | 349578/414113 [01:21<00:14, 4335.39it/s]\u001b[A\n",
      " 85%|████████▍ | 350015/414113 [01:21<00:14, 4345.14it/s]\u001b[A\n",
      " 85%|████████▍ | 350462/414113 [01:21<00:14, 4379.69it/s]\u001b[A\n",
      " 85%|████████▍ | 350926/414113 [01:21<00:14, 4453.77it/s]\u001b[A\n",
      " 85%|████████▍ | 351379/414113 [01:21<00:14, 4473.35it/s]\u001b[A\n",
      " 85%|████████▍ | 351827/414113 [01:21<00:14, 4440.11it/s]\u001b[A\n",
      " 85%|████████▌ | 352272/414113 [01:21<00:13, 4439.01it/s]\u001b[A\n",
      " 85%|████████▌ | 352718/414113 [01:21<00:13, 4444.15it/s]\u001b[A\n",
      " 85%|████████▌ | 353165/414113 [01:21<00:13, 4449.71it/s]\u001b[A\n",
      " 85%|████████▌ | 353611/414113 [01:21<00:13, 4398.44it/s]\u001b[A\n",
      " 85%|████████▌ | 354052/414113 [01:22<00:13, 4367.36it/s]\u001b[A\n",
      " 86%|████████▌ | 354489/414113 [01:22<00:13, 4354.98it/s]\u001b[A\n",
      " 86%|████████▌ | 354938/414113 [01:22<00:13, 4392.88it/s]\u001b[A\n",
      " 86%|████████▌ | 355389/414113 [01:22<00:13, 4426.59it/s]\u001b[A\n",
      " 86%|████████▌ | 355832/414113 [01:22<00:13, 4363.61it/s]\u001b[A\n",
      " 86%|████████▌ | 356269/414113 [01:22<00:13, 4328.49it/s]\u001b[A\n",
      " 86%|████████▌ | 356703/414113 [01:22<00:13, 4263.81it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 357130/414113 [01:22<00:13, 4217.89it/s]\u001b[A\n",
      " 86%|████████▋ | 357553/414113 [01:22<00:13, 4156.00it/s]\u001b[A\n",
      " 86%|████████▋ | 357995/414113 [01:22<00:13, 4231.77it/s]\u001b[A\n",
      " 87%|████████▋ | 358433/414113 [01:23<00:13, 4273.73it/s]\u001b[A\n",
      " 87%|████████▋ | 358882/414113 [01:23<00:12, 4336.30it/s]\u001b[A\n",
      " 87%|████████▋ | 359317/414113 [01:23<00:12, 4319.24it/s]\u001b[A\n",
      " 87%|████████▋ | 359750/414113 [01:23<00:12, 4282.28it/s]\u001b[A\n",
      " 87%|████████▋ | 360179/414113 [01:23<00:12, 4253.10it/s]\u001b[A\n",
      " 87%|████████▋ | 360605/414113 [01:23<00:12, 4211.71it/s]\u001b[A\n",
      " 87%|████████▋ | 361027/414113 [01:23<00:12, 4197.22it/s]\u001b[A\n",
      " 87%|████████▋ | 361447/414113 [01:23<00:12, 4193.26it/s]\u001b[A\n",
      " 87%|████████▋ | 361867/414113 [01:23<00:12, 4176.68it/s]\u001b[A\n",
      " 87%|████████▋ | 362285/414113 [01:23<00:12, 4148.34it/s]\u001b[A\n",
      " 88%|████████▊ | 362700/414113 [01:24<00:12, 4086.97it/s]\u001b[A\n",
      " 88%|████████▊ | 363114/414113 [01:24<00:12, 4101.68it/s]\u001b[A\n",
      " 88%|████████▊ | 363525/414113 [01:24<00:12, 4101.89it/s]\u001b[A\n",
      " 88%|████████▊ | 363936/414113 [01:24<00:12, 4082.98it/s]\u001b[A\n",
      " 88%|████████▊ | 364367/414113 [01:24<00:11, 4148.40it/s]\u001b[A\n",
      " 88%|████████▊ | 364810/414113 [01:24<00:11, 4227.41it/s]\u001b[A\n",
      " 88%|████████▊ | 365258/414113 [01:24<00:11, 4298.42it/s]\u001b[A\n",
      " 88%|████████▊ | 365701/414113 [01:24<00:11, 4335.87it/s]\u001b[A\n",
      " 88%|████████▊ | 366149/414113 [01:24<00:10, 4377.25it/s]\u001b[A\n",
      " 89%|████████▊ | 366588/414113 [01:24<00:10, 4359.34it/s]\u001b[A\n",
      " 89%|████████▊ | 367025/414113 [01:25<00:10, 4345.43it/s]\u001b[A\n",
      " 89%|████████▊ | 367460/414113 [01:25<00:10, 4344.82it/s]\u001b[A\n",
      " 89%|████████▉ | 367909/414113 [01:25<00:10, 4386.29it/s]\u001b[A\n",
      " 89%|████████▉ | 368348/414113 [01:25<00:10, 4382.31it/s]\u001b[A\n",
      " 89%|████████▉ | 368793/414113 [01:25<00:10, 4400.98it/s]\u001b[A\n",
      " 89%|████████▉ | 369236/414113 [01:25<00:10, 4408.54it/s]\u001b[A\n",
      " 89%|████████▉ | 369677/414113 [01:25<00:10, 4386.28it/s]\u001b[A\n",
      " 89%|████████▉ | 370116/414113 [01:25<00:10, 4360.32it/s]\u001b[A\n",
      " 89%|████████▉ | 370553/414113 [01:25<00:10, 4326.58it/s]\u001b[A\n",
      " 90%|████████▉ | 370986/414113 [01:25<00:10, 4295.43it/s]\u001b[A\n",
      " 90%|████████▉ | 371416/414113 [01:26<00:09, 4290.95it/s]\u001b[A\n",
      " 90%|████████▉ | 371846/414113 [01:26<00:09, 4254.30it/s]\u001b[A\n",
      " 90%|████████▉ | 372286/414113 [01:26<00:09, 4294.83it/s]\u001b[A\n",
      " 90%|█████████ | 372717/414113 [01:26<00:09, 4295.07it/s]\u001b[A\n",
      " 90%|█████████ | 373161/414113 [01:26<00:09, 4336.76it/s]\u001b[A\n",
      " 90%|█████████ | 373602/414113 [01:26<00:09, 4357.63it/s]\u001b[A\n",
      " 90%|█████████ | 374046/414113 [01:26<00:09, 4380.88it/s]\u001b[A\n",
      " 90%|█████████ | 374485/414113 [01:26<00:09, 4374.76it/s]\u001b[A\n",
      " 91%|█████████ | 374923/414113 [01:26<00:09, 4151.14it/s]\u001b[A\n",
      " 91%|█████████ | 375357/414113 [01:27<00:09, 4205.44it/s]\u001b[A\n",
      " 91%|█████████ | 375788/414113 [01:27<00:09, 4234.03it/s]\u001b[A\n",
      " 91%|█████████ | 376213/414113 [01:27<00:09, 4199.91it/s]\u001b[A\n",
      " 91%|█████████ | 376635/414113 [01:27<00:09, 4157.40it/s]\u001b[A\n",
      " 91%|█████████ | 377052/414113 [01:27<00:09, 4107.55it/s]\u001b[A\n",
      " 91%|█████████ | 377473/414113 [01:27<00:08, 4137.52it/s]\u001b[A\n",
      " 91%|█████████▏| 377926/414113 [01:27<00:08, 4247.44it/s]\u001b[A\n",
      " 91%|█████████▏| 378377/414113 [01:27<00:08, 4321.72it/s]\u001b[A\n",
      " 91%|█████████▏| 378818/414113 [01:27<00:08, 4345.69it/s]\u001b[A\n",
      " 92%|█████████▏| 379254/414113 [01:27<00:08, 4349.16it/s]\u001b[A\n",
      " 92%|█████████▏| 379692/414113 [01:28<00:07, 4357.72it/s]\u001b[A\n",
      " 92%|█████████▏| 380129/414113 [01:28<00:07, 4292.67it/s]\u001b[A\n",
      " 92%|█████████▏| 380559/414113 [01:28<00:07, 4249.75it/s]\u001b[A\n",
      " 92%|█████████▏| 380985/414113 [01:28<00:07, 4213.08it/s]\u001b[A\n",
      " 92%|█████████▏| 381441/414113 [01:28<00:07, 4311.01it/s]\u001b[A\n",
      " 92%|█████████▏| 381895/414113 [01:28<00:07, 4375.73it/s]\u001b[A\n",
      " 92%|█████████▏| 382351/414113 [01:28<00:07, 4427.50it/s]\u001b[A\n",
      " 92%|█████████▏| 382795/414113 [01:28<00:07, 4393.66it/s]\u001b[A\n",
      " 93%|█████████▎| 383245/414113 [01:28<00:06, 4423.16it/s]\u001b[A\n",
      " 93%|█████████▎| 383692/414113 [01:28<00:06, 4434.95it/s]\u001b[A\n",
      " 93%|█████████▎| 384139/414113 [01:29<00:06, 4444.93it/s]\u001b[A\n",
      " 93%|█████████▎| 384588/414113 [01:29<00:06, 4456.10it/s]\u001b[A\n",
      " 93%|█████████▎| 385034/414113 [01:29<00:06, 4450.28it/s]\u001b[A\n",
      " 93%|█████████▎| 385480/414113 [01:29<00:06, 4435.22it/s]\u001b[A\n",
      " 93%|█████████▎| 385934/414113 [01:29<00:06, 4460.25it/s]\u001b[A\n",
      " 93%|█████████▎| 386388/414113 [01:29<00:06, 4482.14it/s]\u001b[A\n",
      " 93%|█████████▎| 386844/414113 [01:29<00:06, 4503.81it/s]\u001b[A\n",
      " 94%|█████████▎| 387295/414113 [01:29<00:05, 4492.82it/s]\u001b[A\n",
      " 94%|█████████▎| 387745/414113 [01:29<00:05, 4472.21it/s]\u001b[A\n",
      " 94%|█████████▎| 388193/414113 [01:29<00:05, 4444.01it/s]\u001b[A\n",
      " 94%|█████████▍| 388638/414113 [01:30<00:05, 4422.52it/s]\u001b[A\n",
      " 94%|█████████▍| 389090/414113 [01:30<00:05, 4450.82it/s]\u001b[A\n",
      " 94%|█████████▍| 389536/414113 [01:30<00:05, 4441.95it/s]\u001b[A\n",
      " 94%|█████████▍| 389981/414113 [01:30<00:05, 4406.54it/s]\u001b[A\n",
      " 94%|█████████▍| 390422/414113 [01:30<00:05, 4315.10it/s]\u001b[A\n",
      " 94%|█████████▍| 390855/414113 [01:30<00:05, 4228.41it/s]\u001b[A\n",
      " 94%|█████████▍| 391279/414113 [01:30<00:05, 4212.01it/s]\u001b[A\n",
      " 95%|█████████▍| 391710/414113 [01:30<00:05, 4238.34it/s]\u001b[A\n",
      " 95%|█████████▍| 392135/414113 [01:30<00:05, 4239.33it/s]\u001b[A\n",
      " 95%|█████████▍| 392560/414113 [01:30<00:05, 4223.27it/s]\u001b[A\n",
      " 95%|█████████▍| 392983/414113 [01:31<00:05, 4199.10it/s]\u001b[A\n",
      " 95%|█████████▍| 393404/414113 [01:31<00:04, 4180.02it/s]\u001b[A\n",
      " 95%|█████████▌| 393833/414113 [01:31<00:04, 4209.84it/s]\u001b[A\n",
      " 95%|█████████▌| 394255/414113 [01:31<00:04, 4180.71it/s]\u001b[A\n",
      " 95%|█████████▌| 394674/414113 [01:31<00:04, 3994.57it/s]\u001b[A\n",
      " 95%|█████████▌| 395121/414113 [01:31<00:04, 4124.26it/s]\u001b[A\n",
      " 96%|█████████▌| 395560/414113 [01:31<00:04, 4199.16it/s]\u001b[A\n",
      " 96%|█████████▌| 395998/414113 [01:31<00:04, 4251.79it/s]\u001b[A\n",
      " 96%|█████████▌| 396441/414113 [01:31<00:04, 4300.43it/s]\u001b[A\n",
      " 96%|█████████▌| 396893/414113 [01:31<00:03, 4362.29it/s]\u001b[A\n",
      " 96%|█████████▌| 397343/414113 [01:32<00:03, 4400.35it/s]\u001b[A\n",
      " 96%|█████████▌| 397787/414113 [01:32<00:03, 4409.67it/s]\u001b[A\n",
      " 96%|█████████▌| 398229/414113 [01:32<00:03, 4333.37it/s]\u001b[A\n",
      " 96%|█████████▋| 398664/414113 [01:32<00:03, 4285.23it/s]\u001b[A\n",
      " 96%|█████████▋| 399094/414113 [01:32<00:03, 4272.45it/s]\u001b[A\n",
      " 96%|█████████▋| 399522/414113 [01:32<00:03, 4236.91it/s]\u001b[A\n",
      " 97%|█████████▋| 399947/414113 [01:32<00:03, 4195.87it/s]\u001b[A\n",
      " 97%|█████████▋| 400367/414113 [01:32<00:03, 4185.51it/s]\u001b[A\n",
      " 97%|█████████▋| 400786/414113 [01:32<00:03, 4180.50it/s]\u001b[A\n",
      " 97%|█████████▋| 401228/414113 [01:33<00:03, 4248.21it/s]\u001b[A\n",
      " 97%|█████████▋| 401662/414113 [01:33<00:02, 4273.84it/s]\u001b[A\n",
      " 97%|█████████▋| 402103/414113 [01:33<00:02, 4311.37it/s]\u001b[A\n",
      " 97%|█████████▋| 402542/414113 [01:33<00:02, 4333.13it/s]\u001b[A\n",
      " 97%|█████████▋| 402976/414113 [01:33<00:02, 4285.60it/s]\u001b[A\n",
      " 97%|█████████▋| 403405/414113 [01:33<00:02, 4228.99it/s]\u001b[A\n",
      " 98%|█████████▊| 403829/414113 [01:33<00:02, 4190.34it/s]\u001b[A\n",
      " 98%|█████████▊| 404254/414113 [01:33<00:02, 4205.37it/s]\u001b[A\n",
      " 98%|█████████▊| 404683/414113 [01:33<00:02, 4230.18it/s]\u001b[A\n",
      " 98%|█████████▊| 405107/414113 [01:33<00:02, 4210.93it/s]\u001b[A\n",
      " 98%|█████████▊| 405529/414113 [01:34<00:02, 4177.80it/s]\u001b[A\n",
      " 98%|█████████▊| 405947/414113 [01:34<00:01, 4150.81it/s]\u001b[A\n",
      " 98%|█████████▊| 406363/414113 [01:34<00:01, 4123.21it/s]\u001b[A\n",
      " 98%|█████████▊| 406795/414113 [01:34<00:01, 4180.23it/s]\u001b[A\n",
      " 98%|█████████▊| 407225/414113 [01:34<00:01, 4214.29it/s]\u001b[A\n",
      " 98%|█████████▊| 407647/414113 [01:34<00:01, 4201.88it/s]\u001b[A\n",
      " 99%|█████████▊| 408068/414113 [01:34<00:01, 4159.94it/s]\u001b[A\n",
      " 99%|█████████▊| 408500/414113 [01:34<00:01, 4204.46it/s]\u001b[A\n",
      " 99%|█████████▊| 408921/414113 [01:34<00:01, 4185.18it/s]\u001b[A\n",
      " 99%|█████████▉| 409349/414113 [01:34<00:01, 4211.93it/s]\u001b[A\n",
      " 99%|█████████▉| 409788/414113 [01:35<00:01, 4261.50it/s]\u001b[A\n",
      " 99%|█████████▉| 410231/414113 [01:35<00:00, 4310.12it/s]\u001b[A\n",
      " 99%|█████████▉| 410671/414113 [01:35<00:00, 4336.51it/s]\u001b[A\n",
      " 99%|█████████▉| 411129/414113 [01:35<00:00, 4404.77it/s]\u001b[A\n",
      " 99%|█████████▉| 411572/414113 [01:35<00:00, 4410.82it/s]\u001b[A\n",
      " 99%|█████████▉| 412018/414113 [01:35<00:00, 4423.10it/s]\u001b[A\n",
      "100%|█████████▉| 412461/414113 [01:35<00:00, 4411.68it/s]\u001b[A\n",
      "100%|█████████▉| 412903/414113 [01:36<00:00, 2286.70it/s]\u001b[A\n",
      "100%|█████████▉| 413351/414113 [01:36<00:00, 2679.68it/s]\u001b[A\n",
      "100%|█████████▉| 413775/414113 [01:36<00:00, 3011.66it/s]\u001b[A\n",
      "100%|██████████| 414113/414113 [01:36<00:00, 4299.11it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done (t=0.94s)\n",
      "creating index...\n",
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    }
   ],
   "source": [
    "# Obtain the data loader.\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行上面的代码单元格时，数据加载器会存储在变量`data_loader`中。\n",
    "\n",
    "你可以将相应的数据集以`data_loader.dataset` 的方式访问。 此数据集是**data_loader.py**中`CoCoDataset`类的一个实例。 如果你对数据加载器和数据集感到陌生，我们建议你查看 [ 此 PyTorch 教程 ](http://pytorch.org/tutorials/beginner/data_loading_tutorial.html)。\n",
    "\n",
    "### 了解 `__getitem__` 方法\n",
    "\n",
    " `CoCoDataset`类中的`__getitem__`方法用于确定图像标注对在合并到批处理之前应如何进行预处理。 PyTorch中的所有`Dataset` 类都是如此。如果你此感到陌生，请查看 [上面链接中的教程 ](http://pytorch.org/tutorials/beginner/data_loading_tutorial.html)。\n",
    "\n",
    "当数据加载器处于训练模式时，该方法将首先获得训练图像的文件名（`path`）及其对应的标注（`caption`）。\n",
    "\n",
    "#### 图像预处理 \n",
    "\n",
    "图像预处理相对比较简单（来自`CoCoDataset`类中的`__getitem__`方法）："
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Convert image to tensor and pre-process using transform\n",
    "image = Image.open(os.path.join(self.img_folder, path)).convert('RGB')\n",
    "image = self.transform(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将训练文件夹`path`中的图像进行加载后，你需要使用与在实例化数据加载器时相同的转换方法（`transform_train`）对这些图像进行预处理。\n",
    "\n",
    "#### 标注预处理 \n",
    "\n",
    "图像的标注也需要进行预处理，并为训练做好准备。 在这个例子中，为了生成图像标注，我们的目标是创建一个模型，该模型是用于根据一个句子的前一个token预测下一个token。因此，我们要把与所有图像相关联的标注转换为标记化单词列表，然后将其转换为可用于训练网络的PyTorch张量。\n",
    "\n",
    "为了更详细地了解COCO描述是如何进行预处理的，我们首先需要看一下`CoCoDataset`类的`vocab`实例变量。下面的代码片段是从 `CoCoDataset`类中的`__init__`方法中提取的："
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def __init__(self, transform, mode, batch_size, vocab_threshold, vocab_file, start_word, \n",
    "        end_word, unk_word, annotations_file, vocab_from_file, img_folder):\n",
    "        ...\n",
    "        self.vocab = Vocabulary(vocab_threshold, vocab_file, start_word,\n",
    "            end_word, unk_word, annotations_file, vocab_from_file)\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面的代码片段中，你可以看到，`data_loader.dataset.vocab`是**vocabulary.py**中`Vocabulary` 类的一个实例。 现在，花几分钟的时间查看一下**data_loader.py**中的完整代码，然后自行验证一下吧。\n",
    "\n",
    "接下来，我们要使用这个实例对COCO描述进行预处理（来自`CoCoDataset`类中的`__getitem__`方法）："
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Convert caption to tensor of word ids.\n",
    "tokens = nltk.tokenize.word_tokenize(str(caption).lower())   # line 1\n",
    "caption = []                                  # line 2\n",
    "caption.append(self.vocab(self.vocab.start_word))          # line 3\n",
    "caption.extend([self.vocab(token) for token in tokens])      # line 4\n",
    "caption.append(self.vocab(self.vocab.end_word))           # line 5\n",
    "caption = torch.Tensor(caption).long()                 # line 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你很快就会看到，此代码会将所有字符串值的标注转换为整数列表，然后再将其转换为PyTorch张量。 为了弄清楚此代码的工作原理，我们将其应用于下一个代码单元格中的示例标注。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_caption = 'A person doing a trick on a rail while riding a skateboard.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在代码片段的**`line 1`**中，标注中的每个字母都转换为小写，且[`nltk.tokenize.word_tokenize`](http://www.nltk.org/) 函数用于获取字符串值token的列表。 运行下一个代码单元格，将其对`sample_caption`的影响可视化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'person', 'doing', 'a', 'trick', 'on', 'a', 'rail', 'while', 'riding', 'a', 'skateboard', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "sample_tokens = nltk.tokenize.word_tokenize(str(sample_caption).lower())\n",
    "print(sample_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在**`line 2`**和**`line 3`**中，我们初始化一个空列表并附加一个整数来标记一个图像标注的开头。 我们建议你阅读的 [这篇论文](https://arxiv.org/pdf/1411.4555.pdf) 使用了一个特殊的起始单词（与一个特殊的结束单词，我们将在下面查看）来标记一个标注的开头（和结尾）。\n",
    "\n",
    "这个特殊的起始单词（`\"<start>\"`）是在实例化数据加载器时确定的，并作为参数（`start_word`）传递。 你**需要**将此参数保持为其默认值（`start_word=\"<start>\"`）。\n",
    "\n",
    "你将在下面看到，整数`0`始终用于标记一个标注的开头。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special start word: <start>\n"
     ]
    }
   ],
   "source": [
    "sample_caption = []\n",
    "\n",
    "start_word = data_loader.dataset.vocab.start_word\n",
    "print('Special start word:', start_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "sample_caption.append(data_loader.dataset.vocab(start_word))\n",
    "print(sample_caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在**`line 4`**中，我们通过添加与标注中的每个token对应的整数来继续这个列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 3, 98, 754, 3, 396, 39, 3, 1009, 207, 139, 3, 753, 18]\n"
     ]
    }
   ],
   "source": [
    "sample_caption.extend([data_loader.dataset.vocab(token) for token in sample_tokens])\n",
    "print(sample_caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在**`line 5`**，我们附加了最后一个整数，用来标记该标注的结尾。\n",
    "\n",
    "与上面提到的特殊起始单词相同，特殊结束单词（`\"<end>\"`）会在实例化数据加载器时被确定，并作为参数（`end_word`）传递。 你**需要**将此参数保持为其默认值（`end_word=\"<end>\"`）。\n",
    "\n",
    "你将在下面看到，整数`1`始终用于标记一个标注的结尾。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special end word: <end>\n"
     ]
    }
   ],
   "source": [
    "end_word = data_loader.dataset.vocab.end_word\n",
    "print('Special end word:', end_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 3, 98, 754, 3, 396, 39, 3, 1009, 207, 139, 3, 753, 18, 1]\n"
     ]
    }
   ],
   "source": [
    "sample_caption.append(data_loader.dataset.vocab(end_word))\n",
    "print(sample_caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，在**`line 6`**中，我们将整数列表转换为PyTorch张量并将其转换为 [long 类型](http://pytorch.org/docs/master/tensors.html#torch.Tensor.long)。 此外，你可以在 [这个网站](http://pytorch.org/docs/master/tensors.html)上阅读有关不同类型PyTorch张量的更多信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    0,     3,    98,   754,     3,   396,    39,     3,  1009,\n",
      "          207,   139,     3,   753,    18,     1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "sample_caption = torch.Tensor(sample_caption).long()\n",
    "print(sample_caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "就是这样！ 总之，所有标注都会转换为token列表，其中， _特殊的_开始和结束token用来标记句子的开头和结尾，如下所示："
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[<start>, 'a', 'person', 'doing', 'a', 'trick', 'while', 'riding', 'a', 'skateboard', '.', <end>]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后将此token列表转换为整数列表，其中，词汇表中的每个不同单词都具有各自相关联的整数值："
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[0, 3, 98, 754, 3, 396, 207, 139, 3, 753, 18, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，此列表将转换为一个PyTorch张量。 使用上述**`lines 1-6`**的相同步骤对COCO数据集中的所有标注进行预处理。\n",
    "\n",
    "如你所看到的那样，为了将token转换为其对应的整数，我们将`data_loader.dataset.vocab` 称作一个函数。 你可以在**vocabulary.py**中`Vocabulary`类的`__call__`方法中详细了解此call具体是如何工作的。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def __call__(self, word):\n",
    "    if not word in self.word2idx:\n",
    "        return self.word2idx[self.unk_word]\n",
    "    return self.word2idx[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`word2idx`实例变量是一个Python [字典](https://docs.python.org/3/tutorial/datastructures.html#dictionaries) ，它由字符串值键索引，而这些字符串值键主要是从训练标注获得的token。 对于每个键，对应的值是token在预处理步骤中映射到的整数。\n",
    "\n",
    "使用下面的代码单元格查看该字典的子集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the word2idx dictionary.\n",
    "dict(list(data_loader.dataset.vocab.word2idx.items())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此外，我们还输出了键总数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the total number of keys in the word2idx dictionary.\n",
    "print('Total number of tokens in vocabulary:', len(data_loader.dataset.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，你会看到，如果检查**vocabulary.py**中的代码，则通过遍历训练数据集中的图像标注就可以创建一个`word2idx`字典。 如果token在训练集中出现的次数不小于`vocab_threshold`次数，则将其作为键添加到该字典中并分配一个相应的唯一整数。 之后，你可以选择在实例化数据加载器时修改`vocab_threshold`参数。 请注意，通常情况下，**较小的**`vocab_threshold`值会在词汇表中生成**更多的**token。 另外，我们建议你在创建新数据加载器之前减少`vocab_threshold`的值，这样便于在下一个代码单元格中自行检查。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#已经知道结果了，省时间不运行了\n",
    "# Modify the minimum word count threshold.\n",
    "vocab_threshold = 4\n",
    "\n",
    "# Obtain the data loader.\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Print the total number of keys in the word2idx dictionary.\n",
    "print('Total number of tokens in vocabulary:', len(data_loader.dataset.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`word2idx` 字典中还有一些特殊键。 通过前面的内容，你已经熟悉了特殊的起始单词（`\"<start>\"`）和特殊的结束单词（`\"<end>\"`）。在这里，还有一个特殊的token，对应的是未知的单词（`\"<unk>\"`）。 所有未出现在`word2idx`字典中的token都被视为未知单词。 在预处理步骤中，任何未知token都会映射到整数`2`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_word = data_loader.dataset.vocab.unk_word\n",
    "print('Special unknown word:', unk_word)\n",
    "\n",
    "print('All unknown words are mapped to this integer:', data_loader.dataset.vocab(unk_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请在下面通过对从未出现在训练标注中的提供的无意义单词进行预处理，做个自行检查吧。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_loader.dataset.vocab('jfkafejw'))\n",
    "print(data_loader.dataset.vocab('ieowoqjf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后提到的是创建数据加载器时提供的`vocab_from_file`参数。请注意，要理解这个参数，在创建新的数据加载器时，词汇表（`data_loader.dataset.vocab`）需要保存为项目文件夹中的 [pickle](https://docs.python.org/3/library/pickle.html)文件，文件名为`vocab.pkl`。\n",
    "\n",
    "如果你此刻还在调整`vocab_threshold`参数的值，则**必须**设置为`vocab_from_file=False`，这样才能使更改生效。\n",
    "\n",
    "但是，如果你对为`vocab_threshold`参数选定的值感到满意，则只需*再次*使用所选的`vocab_threshold`运行数据加载器即可，这样可以将新词汇表保存到文件中。然后，就可以设置`vocab_from_file=True` 了，这样便于在文件中加载词汇表并加速数据加载器的实例化。请注意，从零开始构建词汇表是实例化数据加载器过程中最耗时的一部分，因此我们强烈建议你尽快设置`vocab_from_file=True`。\n",
    "\n",
    "另外，还需要注意的是，如果`vocab_from_file=True`，则在实例化数据加载器时为`vocab_threshold`提供的任何参数都将被完全忽略。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#上面已经运行，不用运行\n",
    "# Obtain the data loader (from file). Note that it runs much faster than before!\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_from_file=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在下一部分中，你将要学习的是如何使用数据加载器来获取批量训练数据。\n",
    "\n",
    "<a id='step2'></a>\n",
    "## Step 2: 使用数据加载器获取批量数据\n",
    "\n",
    "数据集中的图像标注长度差异很大，查看一下Python列表`data_loader.dataset.caption_lengths`就可以发现这一点。在这个列表中，每个训练标注都有一个entry（其中，值用于存储相应标注的长度）。\n",
    "\n",
    "在下面的代码单元格中，我们使用此列表输出每个长度的训练数据中的标注总数。 接下来你会看到，大多数标注的长度为10。同时，过短与过长的标注非常少见。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value: 10 --- count: 86334\n",
      "value: 11 --- count: 79948\n",
      "value:  9 --- count: 71934\n",
      "value: 12 --- count: 57637\n",
      "value: 13 --- count: 37645\n",
      "value: 14 --- count: 22335\n",
      "value:  8 --- count: 20771\n",
      "value: 15 --- count: 12841\n",
      "value: 16 --- count:  7729\n",
      "value: 17 --- count:  4842\n",
      "value: 18 --- count:  3104\n",
      "value: 19 --- count:  2014\n",
      "value:  7 --- count:  1597\n",
      "value: 20 --- count:  1451\n",
      "value: 21 --- count:   999\n",
      "value: 22 --- count:   683\n",
      "value: 23 --- count:   534\n",
      "value: 24 --- count:   383\n",
      "value: 25 --- count:   277\n",
      "value: 26 --- count:   215\n",
      "value: 27 --- count:   159\n",
      "value: 28 --- count:   115\n",
      "value: 29 --- count:    86\n",
      "value: 30 --- count:    58\n",
      "value: 31 --- count:    49\n",
      "value: 32 --- count:    44\n",
      "value: 34 --- count:    39\n",
      "value: 37 --- count:    32\n",
      "value: 33 --- count:    31\n",
      "value: 35 --- count:    31\n",
      "value: 36 --- count:    26\n",
      "value: 38 --- count:    18\n",
      "value: 39 --- count:    18\n",
      "value: 43 --- count:    16\n",
      "value: 44 --- count:    16\n",
      "value: 48 --- count:    12\n",
      "value: 45 --- count:    11\n",
      "value: 42 --- count:    10\n",
      "value: 40 --- count:     9\n",
      "value: 49 --- count:     9\n",
      "value: 46 --- count:     9\n",
      "value: 47 --- count:     7\n",
      "value: 50 --- count:     6\n",
      "value: 51 --- count:     6\n",
      "value: 41 --- count:     6\n",
      "value: 52 --- count:     5\n",
      "value: 54 --- count:     3\n",
      "value: 56 --- count:     2\n",
      "value:  6 --- count:     2\n",
      "value: 53 --- count:     2\n",
      "value: 55 --- count:     2\n",
      "value: 57 --- count:     1\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Tally the total number of training captions with each length.\n",
    "counter = Counter(data_loader.dataset.caption_lengths)\n",
    "lengths = sorted(counter.items(), key=lambda pair: pair[1], reverse=True)\n",
    "for value, count in lengths:\n",
    "    print('value: %2d --- count: %5d' % (value, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了生成批量的训练数据，我们首先对标注长度进行采样。在采样中，抽取的所有长度的概率需要与数据集中具有该长度的标注的数量成比例。 然后，我们检索一批图像标注对的size`batch_size`，其中，所有标注都具有采样长度。 这种用于分配批次的方法与 [这篇文章 ](https://arxiv.org/pdf/1502.03044.pdf) 中的过程相匹配，并且已被证明在不降低性能的情况下具有计算上的有效性。\n",
    "\n",
    "运行下面的代码单元格，生成一个批次。 `CoCoDataset`类中的`get_train_indices`方法首先对标注长度进行采样，然后对与训练数据点对应的`batch_size`indices进行采样，并使用该长度的标注。 这些indices存储在`indices`下方。\n",
    "\n",
    "这些indices会提供给数据加载器，然后用于检索相应的数据点。该批次中的预处理图像和标注存储在`images`和`captions`中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampled indices: [88203, 403031, 83100, 322256, 79405, 214352, 320275, 113201, 277753, 75225, 337221, 233285, 183655, 307306, 134170, 166091]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "\n",
    "# Randomly sample a caption length, and sample indices with that length.\n",
    "indices = data_loader.dataset.get_train_indices()\n",
    "print('sampled indices:', indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader.dataset.caption_lengths[45311]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images.shape: torch.Size([16, 3, 224, 224])\n",
      "captions.shape: torch.Size([16, 15])\n",
      "images: tensor([[[[-0.5938, -0.5767, -0.5767,  ..., -0.0116,  0.0056,  0.0056],\n",
      "          [-0.5767, -0.5596, -0.5767,  ...,  0.0056,  0.0056,  0.0056],\n",
      "          [-0.5596, -0.5424, -0.5767,  ...,  0.0227,  0.0227,  0.0056],\n",
      "          ...,\n",
      "          [ 0.9303,  0.9303,  0.9303,  ...,  1.1872,  1.1529,  1.1872],\n",
      "          [ 0.9646,  0.9817,  0.9474,  ...,  0.9646,  0.9646,  1.1187],\n",
      "          [ 0.9303,  0.8789,  0.7933,  ...,  0.9303,  0.9474,  0.9817]],\n",
      "\n",
      "         [[ 0.2752,  0.2927,  0.2927,  ...,  0.8529,  0.8704,  0.8704],\n",
      "          [ 0.2927,  0.2752,  0.2927,  ...,  0.8704,  0.8704,  0.8880],\n",
      "          [ 0.2752,  0.2927,  0.3102,  ...,  0.8880,  0.8880,  0.9055],\n",
      "          ...,\n",
      "          [ 1.0280,  1.0280,  1.0280,  ...,  1.2381,  1.2381,  1.2556],\n",
      "          [ 1.0455,  1.0805,  1.0455,  ...,  1.0455,  1.0805,  1.2031],\n",
      "          [ 1.0280,  0.9755,  0.8880,  ...,  1.0105,  1.0630,  1.0980]],\n",
      "\n",
      "         [[ 1.3851,  1.4025,  1.4025,  ...,  1.9080,  1.9254,  1.9428],\n",
      "          [ 1.4025,  1.4025,  1.4025,  ...,  1.9254,  1.9254,  1.9603],\n",
      "          [ 1.4025,  1.4025,  1.4200,  ...,  1.9428,  1.9428,  1.9777],\n",
      "          ...,\n",
      "          [ 1.0365,  1.0365,  1.0888,  ...,  1.2631,  1.2457,  1.2457],\n",
      "          [ 1.1062,  1.1062,  1.1237,  ...,  1.0714,  1.0888,  1.2282],\n",
      "          [ 1.0888,  1.0191,  0.9668,  ...,  1.0191,  1.0539,  1.0888]]],\n",
      "\n",
      "\n",
      "        [[[-0.9534, -0.9877, -0.9705,  ...,  0.6049,  0.6392,  0.6221],\n",
      "          [-1.0219, -1.0562, -1.0048,  ...,  0.5707,  0.6221,  0.5878],\n",
      "          [-1.0562, -1.1760, -1.1589,  ...,  0.6563,  0.6221,  0.6221],\n",
      "          ...,\n",
      "          [ 1.7009,  2.0434,  2.1462,  ...,  1.8722,  1.9578,  1.9064],\n",
      "          [ 2.0434,  2.1975,  2.1975,  ...,  2.0434,  2.0092,  1.8893],\n",
      "          [ 2.2147,  2.2318,  2.2318,  ...,  2.1462,  2.1119,  2.0605]],\n",
      "\n",
      "         [[-0.7402, -0.7752, -0.7752,  ...,  0.8354,  0.8529,  0.8354],\n",
      "          [-0.8102, -0.8452, -0.8102,  ...,  0.8004,  0.8529,  0.8179],\n",
      "          [-0.8277, -0.9503, -0.9328,  ...,  0.8880,  0.8529,  0.8529],\n",
      "          ...,\n",
      "          [ 1.8859,  2.2185,  2.3235,  ...,  2.0784,  2.1485,  2.0959],\n",
      "          [ 2.2185,  2.3761,  2.3761,  ...,  2.2360,  2.2010,  2.0959],\n",
      "          [ 2.3936,  2.4111,  2.4111,  ...,  2.3410,  2.2885,  2.2360]],\n",
      "\n",
      "         [[-0.5844, -0.6541, -0.6541,  ...,  0.9494,  0.9842,  0.9494],\n",
      "          [-0.6367, -0.6890, -0.6715,  ...,  0.9145,  0.9842,  0.9319],\n",
      "          [-0.7064, -0.8110, -0.8110,  ...,  1.0191,  1.0017,  0.9842],\n",
      "          ...,\n",
      "          [ 2.0300,  2.3786,  2.5006,  ...,  2.2217,  2.2740,  2.2391],\n",
      "          [ 2.3960,  2.5529,  2.5877,  ...,  2.4134,  2.3786,  2.2391],\n",
      "          [ 2.6051,  2.6051,  2.6226,  ...,  2.5180,  2.4483,  2.4134]]],\n",
      "\n",
      "\n",
      "        [[[ 1.6667,  1.7009,  1.7523,  ...,  2.1290,  2.1290,  2.1462],\n",
      "          [ 1.6667,  1.6667,  1.7180,  ...,  2.1462,  2.1290,  2.1462],\n",
      "          [ 1.6495,  1.6667,  1.7180,  ...,  2.1290,  2.1290,  2.1462],\n",
      "          ...,\n",
      "          [-0.9020, -1.2274, -1.0904,  ..., -0.1657, -0.2856, -0.3369],\n",
      "          [-0.9192, -1.1075, -1.0390,  ..., -0.3027, -0.2684, -0.2513],\n",
      "          [-0.8678, -0.8849, -0.9534,  ..., -0.3369, -0.1999, -0.1143]],\n",
      "\n",
      "         [[ 1.9909,  2.0084,  2.0259,  ...,  2.3761,  2.3761,  2.3936],\n",
      "          [ 1.9909,  1.9909,  2.0084,  ...,  2.3936,  2.3761,  2.3936],\n",
      "          [ 1.9909,  1.9909,  2.0084,  ...,  2.3761,  2.3761,  2.3936],\n",
      "          ...,\n",
      "          [-1.1779, -1.4230, -1.3179,  ..., -0.5826, -0.7052, -0.7052],\n",
      "          [-1.1779, -1.3179, -1.2654,  ..., -0.7052, -0.6702, -0.6352],\n",
      "          [-1.1604, -1.1429, -1.2129,  ..., -0.7227, -0.6001, -0.5126]],\n",
      "\n",
      "         [[ 2.2914,  2.3088,  2.3088,  ...,  2.6051,  2.6051,  2.6226],\n",
      "          [ 2.3263,  2.3263,  2.3088,  ...,  2.6226,  2.6051,  2.6226],\n",
      "          [ 2.3437,  2.3437,  2.3263,  ...,  2.6051,  2.6051,  2.6226],\n",
      "          ...,\n",
      "          [-1.1944, -1.4036, -1.2467,  ..., -0.8458, -0.8981, -0.8807],\n",
      "          [-1.1944, -1.2990, -1.2293,  ..., -0.9156, -0.8458, -0.8110],\n",
      "          [-1.1770, -1.1247, -1.1944,  ..., -0.8981, -0.8284, -0.7761]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-1.7069, -1.6555, -1.6213,  ..., -1.0904, -1.1932, -0.7650],\n",
      "          [-1.7069, -1.6384, -1.5699,  ..., -1.3130, -1.1932, -0.7993],\n",
      "          [-1.8097, -1.4843, -1.5357,  ..., -0.5596, -0.3712,  0.5536],\n",
      "          ...,\n",
      "          [-0.9192, -0.9705, -0.9020,  ..., -0.8164, -0.7993, -0.8335],\n",
      "          [-0.8507, -0.8678, -0.8335,  ..., -0.6794, -0.7650, -0.9020],\n",
      "          [-0.8678, -0.8849, -0.8507,  ..., -0.6109, -0.7479, -0.9192]],\n",
      "\n",
      "         [[-1.3004, -1.3004, -0.8803,  ...,  0.0126, -0.0574,  0.4153],\n",
      "          [-1.3880, -1.2829, -1.1429,  ..., -0.4601, -0.0574,  0.2752],\n",
      "          [-1.4405, -1.0903, -1.2129,  ...,  0.0301,  0.6604,  1.4482],\n",
      "          ...,\n",
      "          [-0.7927, -0.8452, -0.7927,  ..., -0.7052, -0.7577, -0.7577],\n",
      "          [-0.7577, -0.7402, -0.7052,  ..., -0.5826, -0.6877, -0.8102],\n",
      "          [-0.8102, -0.7752, -0.7402,  ..., -0.5301, -0.6702, -0.8277]],\n",
      "\n",
      "         [[-1.5256, -1.4036, -1.4733,  ..., -1.4384, -1.4210, -1.0201],\n",
      "          [-1.5081, -1.5604, -1.5081,  ..., -1.5256, -1.5430, -1.1247],\n",
      "          [-1.4907, -1.3687, -1.3687,  ..., -0.7413, -1.0550, -0.5844],\n",
      "          ...,\n",
      "          [-1.1247, -1.1596, -1.1073,  ..., -1.0724, -1.0550, -1.0724],\n",
      "          [-1.0201, -1.0724, -1.0550,  ..., -0.9678, -1.0376, -1.0724],\n",
      "          [-1.0898, -1.1944, -1.1421,  ..., -0.9330, -1.0201, -1.0724]]],\n",
      "\n",
      "\n",
      "        [[[-0.6452, -0.4226, -0.8849,  ...,  0.6392,  0.6392,  0.6392],\n",
      "          [-0.8849, -0.5596, -0.6623,  ...,  0.5536,  0.5878,  0.6049],\n",
      "          [-0.8849, -0.7822, -0.7137,  ...,  0.3138,  0.3138,  0.4508],\n",
      "          ...,\n",
      "          [ 0.4851,  0.7248,  0.8961,  ...,  1.1015,  1.1187,  1.1015],\n",
      "          [ 0.5878,  0.5364,  0.5536,  ...,  1.1187,  1.0331,  1.0159],\n",
      "          [ 0.7077,  0.8104,  0.8447,  ...,  1.0159,  0.9132,  0.8447]],\n",
      "\n",
      "         [[-0.6527, -0.3200, -0.8277,  ...,  0.9405,  0.9405,  0.9405],\n",
      "          [-0.8978, -0.4076, -0.4776,  ...,  0.8529,  0.8704,  0.9055],\n",
      "          [-0.8627, -0.7227, -0.6352,  ...,  0.6429,  0.6078,  0.7654],\n",
      "          ...,\n",
      "          [ 0.8529,  1.0805,  1.2381,  ...,  1.4307,  1.4132,  1.4132],\n",
      "          [ 0.9405,  0.8880,  0.9055,  ...,  1.4307,  1.3431,  1.3256],\n",
      "          [ 1.0630,  1.1506,  1.1856,  ...,  1.3256,  1.2381,  1.1506]],\n",
      "\n",
      "         [[-0.6193, -0.1487, -0.6193,  ...,  1.2631,  1.2805,  1.2805],\n",
      "          [-0.9156, -0.2707, -0.2532,  ...,  1.1759,  1.2108,  1.2457],\n",
      "          [-0.8284, -0.5844, -0.4101,  ...,  0.9668,  0.9494,  1.1062],\n",
      "          ...,\n",
      "          [ 1.2631,  1.4722,  1.5942,  ...,  1.8034,  1.8034,  1.7860],\n",
      "          [ 1.4025,  1.3328,  1.3154,  ...,  1.7860,  1.7163,  1.6988],\n",
      "          [ 1.4897,  1.5768,  1.5768,  ...,  1.6988,  1.6117,  1.5420]]],\n",
      "\n",
      "\n",
      "        [[[-0.2513, -0.2171, -0.1314,  ..., -1.1418, -1.2103, -1.2617],\n",
      "          [-0.3712, -0.3541, -0.2856,  ..., -1.0562, -1.1589, -1.1932],\n",
      "          [-0.3369, -0.4226, -0.4054,  ..., -1.0390, -1.0562, -1.0733],\n",
      "          ...,\n",
      "          [ 1.4098,  1.3413,  1.1700,  ...,  1.6324,  1.5810,  1.5810],\n",
      "          [ 1.7523,  1.6838,  1.5297,  ...,  1.6838,  1.6153,  1.6324],\n",
      "          [ 1.7694,  1.7694,  1.7694,  ...,  1.7180,  1.7694,  1.7523]],\n",
      "\n",
      "         [[-0.0049,  0.0126,  0.0826,  ..., -0.7052, -0.6877, -0.6352],\n",
      "          [-0.0924, -0.1275, -0.0749,  ..., -0.6702, -0.6527, -0.6001],\n",
      "          [-0.0574, -0.1625, -0.1800,  ..., -0.6527, -0.6001, -0.5301],\n",
      "          ...,\n",
      "          [ 1.9034,  1.8333,  1.6583,  ...,  2.1134,  2.0609,  2.0609],\n",
      "          [ 2.2710,  2.2010,  2.0434,  ...,  2.1660,  2.0959,  2.1134],\n",
      "          [ 2.2885,  2.2885,  2.2885,  ...,  2.2010,  2.2535,  2.2360]],\n",
      "\n",
      "         [[ 0.1825,  0.1999,  0.2522,  ..., -0.3578, -0.3578, -0.3055],\n",
      "          [ 0.0779,  0.0605,  0.0953,  ..., -0.3578, -0.3404, -0.2707],\n",
      "          [ 0.1128,  0.0082,  0.0082,  ..., -0.3753, -0.2707, -0.2010],\n",
      "          ...,\n",
      "          [ 2.1694,  2.1346,  1.9603,  ...,  2.3960,  2.3437,  2.3437],\n",
      "          [ 2.4831,  2.4308,  2.2914,  ...,  2.4483,  2.3786,  2.3960],\n",
      "          [ 2.4831,  2.5006,  2.5180,  ...,  2.4831,  2.5354,  2.5180]]]])\n",
      "captions: tensor([[    0,     3,   764,    55,    21,   251,     4,  3954,  6688,\n",
      "          1175,     6,   251,   268,    18,     1],\n",
      "        [    0,     3,   330,    13,     2,   139,   319,    39,   257,\n",
      "            13,     3,   513,   698,    18,     1],\n",
      "        [    0,    32,  1972,   576,   270,   417,    32,   615,   191,\n",
      "          1194,    32,   131,   279,    18,     1],\n",
      "        [    0,     3,   953,   363,   139,    54,     3,   269,    39,\n",
      "             3,  1162,    13,   319,    18,     1],\n",
      "        [    0,     3,   253,   472,    77,   283,   360,     3,  2323,\n",
      "          1759,    77,     3,   722,   191,     1],\n",
      "        [    0,    32,   371,    77,    32,  3969,   130,   348,   364,\n",
      "           161,     3,   322,   323,    18,     1],\n",
      "        [    0,     3,   783,   332,   224,    39,   257,    13,     3,\n",
      "          1357,   364,   161,   545,    18,     1],\n",
      "        [    0,     3,   253,   169,   139,     3,   514,   354,    54,\n",
      "             3,   514,   258,   698,    18,     1],\n",
      "        [    0,     3,    98,  7371,    39,   251,  5036,  2536,    21,\n",
      "          1750,    77,    32,   297,    18,     1],\n",
      "        [    0,     3,    98,   170,    77,   121,    13,     3,  1074,\n",
      "           360,     3,  1481,  1558,    18,     1],\n",
      "        [    0,     3,   169,     6,    92,  1403,   102,   851,   207,\n",
      "            32,   169,   491,   334,    18,     1],\n",
      "        [    0,     3,  3208,    39,     3,   269,   185,    21,     3,\n",
      "            98,    53,  1016,   715,    18,     1],\n",
      "        [    0,     3,   335,  4356,   115,   224,    39,     3,  1627,\n",
      "            21,     3,   654,   655,    18,     1],\n",
      "        [    0,     3,   330,    13,  1835,   170,    39,   257,    13,\n",
      "             3,   136,   258,   204,    18,     1],\n",
      "        [    0,     3,   330,    13,    51,    53,    21,     3,   371,\n",
      "           417,   514,   258,  1121,    18,     1],\n",
      "        [    0,     3,   169,   270,   160,   140,    77,    32,  4691,\n",
      "           278,   207,   360,    47,   103,     1]])\n"
     ]
    }
   ],
   "source": [
    "# Obtain the batch.\n",
    "images, captions = next(iter(data_loader))\n",
    "    \n",
    "print('images.shape:', images.shape)\n",
    "print('captions.shape:', captions.shape)\n",
    "\n",
    "# (Optional) Uncomment the lines of code below to print the pre-processed images and captions.\n",
    "print('images:', images)\n",
    "print('captions:', captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每次运行上面的代码单元格时，都会对不同的标注长度进行采样，并返回不同批次的训练数据。多次运行代码单元格，尝试检验一下吧！\n",
    "\n",
    "在接下来的一个notebook（**2_Training.ipynb**）中训练你的模型。我们会将用于生成训练批次的代码提供给你。\n",
    "\n",
    "> 打开下面的一个notebook（**2_Training.ipynb**）之前，我们强烈建议你花些时间熟悉**data_loader.py**和**vocabulary.py**中的代码。本notebook的**Step 1**和**Step 2**主要做了一些基本的介绍并指导你的理解。但是，我们的说明并非详尽无遗，作为项目的一部分，是否要学习如何最好地利用这些文件来完成项目，这就取决于你了。__但你不可以修改 *data_loader.py* 或 *vocabulary.py*中的任何代码哦。__\n",
    "\n",
    "在接下来的步骤中，我们将重点学习如何在PyTorch中指定一个CNN-RNN架构，从而实现最终的图像标注目标。\n",
    "\n",
    "<a id='step3'></a>\n",
    "## Step 3: 使用CNN编码器进行实验\n",
    "\n",
    "运行下面的代码单元格，从**model.py**中导入`EncoderCNN`和`DecoderRNN`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Watch for any changes in model.py, and re-load it automatically.\n",
    "% load_ext autoreload\n",
    "% autoreload 2\n",
    "\n",
    "# Import EncoderCNN and DecoderRNN. \n",
    "from model import EncoderCNN, DecoderRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在下一个代码单元格中，我们定义了一个`device`，你将使用它将PyTorch张量移动到GPU（如果CUDA可用的话）。 在进行下一步之前，运行此代码单元格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行下面的代码单元格，在`encoder`中实例化CNN编码器。\n",
    "\n",
    "然后，该notebook的 **Step 2**中批次的预处理图像会通过编码器，且其输出会存储在`features`中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(features): <class 'torch.Tensor'>\n",
      "features.shape: torch.Size([16, 256])\n"
     ]
    }
   ],
   "source": [
    "# Specify the dimensionality of the image embedding.\n",
    "embed_size = 256\n",
    "\n",
    "#-#-#-# Do NOT modify the code below this line. #-#-#-#\n",
    "\n",
    "# Initialize the encoder. (Optional: Add additional arguments if necessary.)\n",
    "encoder = EncoderCNN(embed_size)\n",
    "\n",
    "# Move the encoder to GPU if CUDA is available.\n",
    "encoder.to(device)\n",
    "    \n",
    "# Move last batch of images (from Step 2) to GPU if CUDA is available.   \n",
    "images = images.to(device)\n",
    "\n",
    "# Pass the images through the encoder.\n",
    "features = encoder(images)\n",
    "\n",
    "print('type(features):', type(features))\n",
    "print('features.shape:', features.shape)\n",
    "\n",
    "# Check that your encoder satisfies some requirements of the project! :D\n",
    "assert type(features)==torch.Tensor, \"Encoder output needs to be a PyTorch Tensor.\" \n",
    "assert (features.shape[0]==batch_size) & (features.shape[1]==embed_size), \"The shape of the encoder output is incorrect.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderCNN(\n",
      "  (resnet): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (4): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "    )\n",
      "    (6): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (5): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "    )\n",
      "    (7): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "    )\n",
      "    (8): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
      "  )\n",
      "  (embed): Linear(in_features=2048, out_features=256, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们提供给你的编码器使用预先训练的ResNet-50架构（删除了最终的完全连接层）从一批预处理图像中提取特征。然后将输出展平为矢量，然后通过 `Linear`层，将特征向量转换为与单词向量同样大小的向量。\n",
    "\n",
    "![Encoder](images/encoder.png)\n",
    "\n",
    "为了试验其他架构，我们欢迎并鼓励你修改 **model.py**中的编码器。我们特别建议你 [ 使用一个不同的预训练模型架构 ](http://pytorch.org/docs/master/torchvision/models.html)。当然了，你也可以考虑[ 添加批次归一化 ](http://pytorch.org/docs/master/nn.html#normalization-layers)。\n",
    "\n",
    "> 但是，你**无**需更改编码器的任何内容。\n",
    "\n",
    "对于此项目，你**必须**将预先训练好的CNN合并到编码器中。`EncoderCNN`类必须将`embed_size`作为一个输入参数，这个参数也将对应于你将在 Step 4 中实现的RNN解码器输入的维度。在接下来的notebook中训练模型时（**2_Training.ipynb**） ，我们鼓励你对`embed_size`的值进行调整哦。\n",
    "\n",
    "如果你决定修改`EncoderCNN`类，请保存**model.py**并重新执行上面的代码单元格。如果该代码单元格返回一个断言错误，请在进行下一步之前按照说明修改代码。这个断言错误可以确保`features`具有一个形状为`[batch_size, embed_size]`的PyTorch张量。\n",
    "\n",
    "<a id='step4'></a>\n",
    "## Step 4: 实现RNN解码器\n",
    "\n",
    "在执行下一个代码单元格之前，必须在**model.py**中的`DecoderRNN` 类中编写`__init__`和 `forward`方法。 （ **不要**编写`sample`方法，但到 notebook **3_Inference.ipynb**时，可以使用此方法。）\n",
    "\n",
    ">  `DecoderRNN`类中`__init__`的和 `forward`方法是你需要在此notebook中修改的唯一内容。你将在接下来出现的notebook中编写更多实现方式。\n",
    "\n",
    "你的解码器将会是`DecoderRNN`类的一个实例，且必须接收下列输入：\n",
    "- 包含嵌入图像特征的PyTorch张量`features`（在 Step 3 中输出，当 Step 2 中的最后一批图像通过编码器时）\n",
    "- 与 Step 2中最后一批标注（`captions`）相对应的PyTorch张量。\n",
    "\n",
    "请注意，我们编写数据加载器的方式应该会简化你的代码。特别是，每个训练批次都包含预处理的标注，其中所有标注都具有相同的长度（`captions.shape[1]`），因此**你无需担心填充问题**。\n",
    "> 虽然我们鼓励你实现 [本文](https://arxiv.org/pdf/1411.4555.pdf)中描述的解码器，但仍然希望你实现自行选择的任何一种架构，只要至少使用一个RNN层，且隐藏维度为`hidden_size`。\n",
    "\n",
    "虽然你将使用当前存储在notebook中的最后一个批次来测试该解码器，但你的解码器应编写为接收嵌入图像特征和预处理标注的任意批次作为输入，其中所有标注具有相同的长度。\n",
    "\n",
    "![Decoder](images/decoder.png)\n",
    "\n",
    " 在下面的代码单元格中，`outputs`应该是一个大小为`[batch_size, captions.shape[1], vocab_size]`的PyTorch张量。这样设计输出的目的是`outputs[i,j,k]`包含模型的预测分数，而该分数表示批次中第 `i`个标注中的第`j`个token是词汇表中第`k`个token的可能性。在接下来的notebook（**2_Training.ipynb**）中，我们会提供代码，将这些分数提供给PyTorch中的 [`torch.nn.CrossEntropyLoss`](http://pytorch.org/docs/master/nn.html#torch.nn.CrossEntropyLoss) 优化程序。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8855"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify the number of features in the hidden state of the RNN decoder.\n",
    "hidden_size = 512\n",
    "\n",
    "vocab_size = len(data_loader.dataset.vocab)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(outputs): <class 'torch.Tensor'>\n",
      "outputs.shape: torch.Size([16, 15, 8855])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "# Specify the number of features in the hidden state of the RNN decoder.\n",
    "hidden_size = 512\n",
    "\n",
    "#-#-#-# Do NOT modify the code below this line. #-#-#-#\n",
    "\n",
    "# Store the size of the vocabulary.\n",
    "vocab_size = len(data_loader.dataset.vocab)\n",
    "\n",
    "# Initialize the decoder.\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "# Move the decoder to GPU if CUDA is available.\n",
    "decoder.to(device)\n",
    "    \n",
    "# Move last batch of captions (from Step 1) to GPU if CUDA is available \n",
    "captions = captions.to(device)\n",
    "\n",
    "# Pass the encoder output and captions through the decoder.\n",
    "outputs = decoder(features, captions)\n",
    "\n",
    "print('type(outputs):', type(outputs))\n",
    "print('outputs.shape:', outputs.shape)\n",
    "\n",
    "# Check that your decoder satisfies some requirements of the project! :D\n",
    "assert type(outputs)==torch.Tensor, \"Decoder output needs to be a PyTorch Tensor.\"\n",
    "assert (outputs.shape[0]==batch_size) & (outputs.shape[1]==captions.shape[1]) & (outputs.shape[2]==vocab_size), \"The shape of the decoder output is incorrect.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在接下来的notebook中训练模型时（**2_Training.ipynb**） ，我们鼓励你对`embed_size`的值进行调整哦。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
